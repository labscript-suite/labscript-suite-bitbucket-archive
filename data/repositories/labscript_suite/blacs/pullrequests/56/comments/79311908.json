{"links": {"self": {"href": "data/repositories/labscript_suite/blacs/pullrequests/56/comments/79311908.json"}, "html": {"href": "#!/labscript_suite/blacs/pull-requests/56/_/diff#comment-79311908"}}, "parent": {"id": 79306295, "links": {"self": {"href": "data/repositories/labscript_suite/blacs/pullrequests/56/comments/79306295.json"}, "html": {"href": "#!/labscript_suite/blacs/pull-requests/56/_/diff#comment-79306295"}}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 56, "links": {"self": {"href": "data/repositories/labscript_suite/blacs/pullrequests/56.json"}, "html": {"href": "#!/labscript_suite/blacs/pull-requests/56"}}, "title": "Defer starting the worker process until _initialise_worker"}, "content": {"raw": "I just made that change today - it was because the workers would be initialised in the reverse order than they were created in `initialise_GUI()`, which was unexpected.\n\nI noticed this because in the generic DAQmx class, the primary worker calls DAQmxResetDevice or something like that, which clears all tasks and any other configuration previously set. Problem was, this worker would be initialised last (despite being the first worker configured in `initialise_GUI()`), and so would clear the tasks set up by the other workers. It's not clear to me why I just noticed this problem now, because it should have been  an issue earlier still (in the Spielman fork at least which had the reset call).\n\nI haven't noticed any problems, no. I only tested with a Pulseblaster and a few DAQmx devices, though they all have output widgets and the Pulseblaster has a status monitor timeout obviously. I'll have a poke around to investigate the issues you raised, to see why it didn't cause a problem when it sounds like it ought to have.\n\nIf we need higher priority for some tasks but want to preserve the order multiple high-priority tasks were queued up in, then we might need to give the queue that functionality.", "markup": "markdown", "html": "<p>I just made that change today - it was because the workers would be initialised in the reverse order than they were created in <code>initialise_GUI()</code>, which was unexpected.</p>\n<p>I noticed this because in the generic DAQmx class, the primary worker calls DAQmxResetDevice or something like that, which clears all tasks and any other configuration previously set. Problem was, this worker would be initialised last (despite being the first worker configured in <code>initialise_GUI()</code>), and so would clear the tasks set up by the other workers. It's not clear to me why I just noticed this problem now, because it should have been  an issue earlier still (in the Spielman fork at least which had the reset call).</p>\n<p>I haven't noticed any problems, no. I only tested with a Pulseblaster and a few DAQmx devices, though they all have output widgets and the Pulseblaster has a status monitor timeout obviously. I'll have a poke around to investigate the issues you raised, to see why it didn't cause a problem when it sounds like it ought to have.</p>\n<p>If we need higher priority for some tasks but want to preserve the order multiple high-priority tasks were queued up in, then we might need to give the queue that functionality.</p>", "type": "rendered"}, "created_on": "2018-10-17T00:20:05.757479+00:00", "user": {"display_name": "Chris Billington", "uuid": "{e363c5a9-5075-4656-afb5-88bd6a6dceeb}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D"}, "html": {"href": "https://bitbucket.org/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/9238baf7300c41c0e7294db922899e6ad=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsCB-1.png"}}, "nickname": "cbillington", "type": "user", "account_id": "557058:cbf1bc43-1dc2-477b-9e25-1a8f40fd7ee3"}, "updated_on": "2018-10-17T00:26:06.853365+00:00", "type": "pullrequest_comment", "id": 79311908}