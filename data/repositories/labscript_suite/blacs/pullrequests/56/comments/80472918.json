{"links": {"self": {"href": "data/repositories/labscript_suite/blacs/pullrequests/56/comments/80472918.json"}, "html": {"href": "#!/labscript_suite/blacs/pull-requests/56/_/diff#comment-80472918"}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 56, "links": {"self": {"href": "data/repositories/labscript_suite/blacs/pullrequests/56.json"}, "html": {"href": "#!/labscript_suite/blacs/pull-requests/56"}}, "title": "Defer starting the worker process until _initialise_worker"}, "content": {"raw": "Hey, we have implemented this PR into our branch and encounter the following problem:\n\nWhen using a large amount of worker processes \\(e.g. one PulseBlaster and 5 NI-Cards\\) Blacs fails to initialise all of them. The error message is:\n\n    Fatal exception in main process - Fri Oct 26, 16:48:21 :\n     Traceback (most recent call last):\n      File \"C:\\labscript_suite\\blacs\\tab_base_classes.py\", line 755, in mainloop\n        to_worker, from_worker = worker.start(*worker_args)\n      File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zprocess\\process_tree.py\", line 686, in start\n        response = self.from_child.get(timeout=5)\n      File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zprocess\\process_tree.py\", line 312, in get\n        raise TimeoutError('get() timed out')\n    zprocess.utils.TimeoutError: get() timed out\n    \n    \n\nWe further investigated this and realised the following:\n\n* Hitting restart on any of the failed workers will get them to start normally\n* Some of the workers start at random instances of restarting BLACS.\n* When just using one PulseBlaster and one NI Card, there are no problems\n* all 8 CPU cores go to maximum during worker creation\n\nThis leads us to the conclusion that starting all workers simultaneusly causes problems in combination with the timeout in zprocess of \u201cjust\u201d 5 seconds. Any idea how to resolve this?\n\nLars and Jan ", "markup": "markdown", "html": "<p>Hey, we have implemented this PR into our branch and encounter the following problem:</p>\n<p>When using a large amount of worker processes (e.g. one PulseBlaster and 5 NI-Cards) Blacs fails to initialise all of them. The error message is:</p>\n<div class=\"codehilite\"><pre><span></span>Fatal exception in main process - Fri Oct 26, 16:48:21 :\n Traceback (most recent call last):\n  File &quot;C:\\labscript_suite\\blacs\\tab_base_classes.py&quot;, line 755, in mainloop\n    to_worker, from_worker = worker.start(*worker_args)\n  File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zprocess\\process_tree.py&quot;, line 686, in start\n    response = self.from_child.get(timeout=5)\n  File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zprocess\\process_tree.py&quot;, line 312, in get\n    raise TimeoutError(&#39;get() timed out&#39;)\nzprocess.utils.TimeoutError: get() timed out\n</pre></div>\n\n\n<p>We further investigated this and realised the following:</p>\n<ul>\n<li>Hitting restart on any of the failed workers will get them to start normally</li>\n<li>Some of the workers start at random instances of restarting BLACS.</li>\n<li>When just using one PulseBlaster and one NI Card, there are no problems</li>\n<li>all 8 CPU cores go to maximum during worker creation</li>\n</ul>\n<p>This leads us to the conclusion that starting all workers simultaneusly causes problems in combination with the timeout in zprocess of \u201cjust\u201d 5 seconds. Any idea how to resolve this?</p>\n<p>Lars and Jan </p>", "type": "rendered"}, "created_on": "2018-10-26T15:04:01.090559+00:00", "user": {"display_name": "Lars Kohfahl", "uuid": "{14b59397-cbc0-4d8c-8a4a-fe99fb4d2d4a}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B14b59397-cbc0-4d8c-8a4a-fe99fb4d2d4a%7D"}, "html": {"href": "https://bitbucket.org/%7B14b59397-cbc0-4d8c-8a4a-fe99fb4d2d4a%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/630642264cd55e22515678a3a0489ac7d=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsLK-2.png"}}, "nickname": "lkohfahl", "type": "user", "account_id": "5aafc5d11396802a57aa7f3b"}, "updated_on": "2018-10-26T15:04:01.212419+00:00", "type": "pullrequest_comment", "id": 80472918}