{"links": {"self": {"href": "data/repositories/labscript_suite/blacs/pullrequests/56/comments/80474248.json"}, "html": {"href": "#!/labscript_suite/blacs/pull-requests/56/_/diff#comment-80474248"}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 56, "links": {"self": {"href": "data/repositories/labscript_suite/blacs/pullrequests/56.json"}, "html": {"href": "#!/labscript_suite/blacs/pull-requests/56"}}, "title": "Defer starting the worker process until _initialise_worker"}, "content": {"raw": "Thanks for the report.\n\nSo I think this problems stems from the fact that worker initialisation used to be serialised, i.e. BLACS would not start a worker process until it had completed starting the previous one - including Python interpreter startup and all imports etc in the subprocess, which can take a few seconds. Now, BLACS is starting all the workers at the same time. Although I'm sure this is faster in total, it stands to reason that the time between any one worker starting up and completing its startup will be longer, as windows is doing them all at once and will randomly context switch.\n\nI suspect the solution is just to increase the timeout. I'll check, if this timeout is hard-coded in zprocess, I'll expose it so that BLACS can pass in a higher timeout, say, 30 seconds.\n\nThere is a lot of inefficiency in the worker creation due to us keeping things all the classes in the same files in labscript_devices, leading to a lot of unnecessary imports, but now that there are some changes there that allow things to be split up, worker creation will get faster. If we can avoid importing matplitlib and scipy in the worker processes, then probably you wouldn't see those timeouts. But that's an ongoing task, so for the moment I'll just increase the timeout.", "markup": "markdown", "html": "<p>Thanks for the report.</p>\n<p>So I think this problems stems from the fact that worker initialisation used to be serialised, i.e. BLACS would not start a worker process until it had completed starting the previous one - including Python interpreter startup and all imports etc in the subprocess, which can take a few seconds. Now, BLACS is starting all the workers at the same time. Although I'm sure this is faster in total, it stands to reason that the time between any one worker starting up and completing its startup will be longer, as windows is doing them all at once and will randomly context switch.</p>\n<p>I suspect the solution is just to increase the timeout. I'll check, if this timeout is hard-coded in zprocess, I'll expose it so that BLACS can pass in a higher timeout, say, 30 seconds.</p>\n<p>There is a lot of inefficiency in the worker creation due to us keeping things all the classes in the same files in labscript_devices, leading to a lot of unnecessary imports, but now that there are some changes there that allow things to be split up, worker creation will get faster. If we can avoid importing matplitlib and scipy in the worker processes, then probably you wouldn't see those timeouts. But that's an ongoing task, so for the moment I'll just increase the timeout.</p>", "type": "rendered"}, "created_on": "2018-10-26T15:12:10.537362+00:00", "user": {"display_name": "Chris Billington", "uuid": "{e363c5a9-5075-4656-afb5-88bd6a6dceeb}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D"}, "html": {"href": "https://bitbucket.org/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/9238baf7300c41c0e7294db922899e6ad=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsCB-1.png"}}, "nickname": "cbillington", "type": "user", "account_id": "557058:cbf1bc43-1dc2-477b-9e25-1a8f40fd7ee3"}, "updated_on": "2018-10-26T15:12:10.551920+00:00", "type": "pullrequest_comment", "id": 80474248}