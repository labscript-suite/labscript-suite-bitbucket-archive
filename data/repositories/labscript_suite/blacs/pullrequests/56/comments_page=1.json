{"pagelen": 100, "values": [{"links": {"self": {"href": "data/repositories/labscript_suite/blacs/pullrequests/56/comments/79306295.json"}, "html": {"href": "#!/labscript_suite/blacs/pull-requests/56/_/diff#comment-79306295"}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 56, "links": {"self": {"href": "data/repositories/labscript_suite/blacs/pullrequests/56.json"}, "html": {"href": "#!/labscript_suite/blacs/pull-requests/56"}}, "title": "Defer starting the worker process until _initialise_worker"}, "content": {"raw": "In `create_worker`, the `prepend` keyword is now set to `False` instead of `True` when queuing up the `_initialse_worker` state. Was there a reason for this change? I believe I made it `True` originally so that we guaranteed that it would be the first state to be processed by the `mainloop` even if something you did prior to creating the worker caused states to be queued up \\(for example, doesn't creating output objects and status monitor timeouts trigger states to be queued up?\\). So I'm surprised you haven't seen issues with the worker not being initialised soon enough.", "markup": "markdown", "html": "<p>In <code>create_worker</code>, the <code>prepend</code> keyword is now set to <code>False</code> instead of <code>True</code> when queuing up the <code>_initialse_worker</code> state. Was there a reason for this change? I believe I made it <code>True</code> originally so that we guaranteed that it would be the first state to be processed by the <code>mainloop</code> even if something you did prior to creating the worker caused states to be queued up (for example, doesn't creating output objects and status monitor timeouts trigger states to be queued up?). So I'm surprised you haven't seen issues with the worker not being initialised soon enough.</p>", "type": "rendered"}, "created_on": "2018-10-16T22:34:22.810133+00:00", "user": {"display_name": "Philip Starkey", "uuid": "{48af65db-e5fc-459c-a7eb-52eb1f9a5690}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B48af65db-e5fc-459c-a7eb-52eb1f9a5690%7D"}, "html": {"href": "https://bitbucket.org/%7B48af65db-e5fc-459c-a7eb-52eb1f9a5690%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/dc318537facc47ebe1ae98a7aabacecfd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsPS-0.png"}}, "nickname": "pstarkey", "type": "user", "account_id": "557058:52a111e4-40da-4441-9143-417f95f2db97"}, "updated_on": "2018-10-16T22:34:22.817081+00:00", "type": "pullrequest_comment", "id": 79306295}, {"links": {"self": {"href": "data/repositories/labscript_suite/blacs/pullrequests/56/comments/79311908.json"}, "html": {"href": "#!/labscript_suite/blacs/pull-requests/56/_/diff#comment-79311908"}}, "parent": {"id": 79306295, "links": {"self": {"href": "data/repositories/labscript_suite/blacs/pullrequests/56/comments/79306295.json"}, "html": {"href": "#!/labscript_suite/blacs/pull-requests/56/_/diff#comment-79306295"}}, "depth": 1}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 56, "links": {"self": {"href": "data/repositories/labscript_suite/blacs/pullrequests/56.json"}, "html": {"href": "#!/labscript_suite/blacs/pull-requests/56"}}, "title": "Defer starting the worker process until _initialise_worker"}, "content": {"raw": "I just made that change today - it was because the workers would be initialised in the reverse order than they were created in `initialise_GUI()`, which was unexpected.\n\nI noticed this because in the generic DAQmx class, the primary worker calls DAQmxResetDevice or something like that, which clears all tasks and any other configuration previously set. Problem was, this worker would be initialised last (despite being the first worker configured in `initialise_GUI()`), and so would clear the tasks set up by the other workers. It's not clear to me why I just noticed this problem now, because it should have been  an issue earlier still (in the Spielman fork at least which had the reset call).\n\nI haven't noticed any problems, no. I only tested with a Pulseblaster and a few DAQmx devices, though they all have output widgets and the Pulseblaster has a status monitor timeout obviously. I'll have a poke around to investigate the issues you raised, to see why it didn't cause a problem when it sounds like it ought to have.\n\nIf we need higher priority for some tasks but want to preserve the order multiple high-priority tasks were queued up in, then we might need to give the queue that functionality.", "markup": "markdown", "html": "<p>I just made that change today - it was because the workers would be initialised in the reverse order than they were created in <code>initialise_GUI()</code>, which was unexpected.</p>\n<p>I noticed this because in the generic DAQmx class, the primary worker calls DAQmxResetDevice or something like that, which clears all tasks and any other configuration previously set. Problem was, this worker would be initialised last (despite being the first worker configured in <code>initialise_GUI()</code>), and so would clear the tasks set up by the other workers. It's not clear to me why I just noticed this problem now, because it should have been  an issue earlier still (in the Spielman fork at least which had the reset call).</p>\n<p>I haven't noticed any problems, no. I only tested with a Pulseblaster and a few DAQmx devices, though they all have output widgets and the Pulseblaster has a status monitor timeout obviously. I'll have a poke around to investigate the issues you raised, to see why it didn't cause a problem when it sounds like it ought to have.</p>\n<p>If we need higher priority for some tasks but want to preserve the order multiple high-priority tasks were queued up in, then we might need to give the queue that functionality.</p>", "type": "rendered"}, "created_on": "2018-10-17T00:20:05.757479+00:00", "user": {"display_name": "Chris Billington", "uuid": "{e363c5a9-5075-4656-afb5-88bd6a6dceeb}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D"}, "html": {"href": "https://bitbucket.org/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/9238baf7300c41c0e7294db922899e6ad=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsCB-1.png"}}, "nickname": "cbillington", "type": "user", "account_id": "557058:cbf1bc43-1dc2-477b-9e25-1a8f40fd7ee3"}, "updated_on": "2018-10-17T00:26:06.853365+00:00", "type": "pullrequest_comment", "id": 79311908}, {"links": {"self": {"href": "data/repositories/labscript_suite/blacs/pullrequests/56/comments/79313211.json"}, "html": {"href": "#!/labscript_suite/blacs/pull-requests/56/_/diff#comment-79313211"}}, "parent": {"id": 79311908, "links": {"self": {"href": "data/repositories/labscript_suite/blacs/pullrequests/56/comments/79311908.json"}, "html": {"href": "#!/labscript_suite/blacs/pull-requests/56/_/diff#comment-79311908"}}, "depth": 2}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 56, "links": {"self": {"href": "data/repositories/labscript_suite/blacs/pullrequests/56.json"}, "html": {"href": "#!/labscript_suite/blacs/pull-requests/56"}}, "title": "Defer starting the worker process until _initialise_worker"}, "content": {"raw": "So it looks like the Pulseblaster's status monitor timeout is added after its worker is configured, and `program_manual()` is called by `DeviceTab.__init__` only after `initialise_GUI()` is called. \n\nNeither the DAQmx tab or the PulseBlaster use `initialise_workers()`, they put their worker initialisation in `initialise_GUI()`. Even if they did use `initialise_workers()`, it still runs before the first `program_manual()`.\n\nSo that's why there's no issue so far.\n\nMany of our devices don't use `initialise_workers()`, they just put everything in `initialise_GUI()`. Looks like the only difference is that save data is restored after `initialise_GUI()` but before `initialise_workers()`, so if you're using the save data in the initialisation of the worker, but it also needs to be put in the (previously set up) GUI, then you have to do them separately. Maybe I should change the DAQmx class to use `initialise_workers()` as a better example. Making the PulseBlaster use it would only work if the `statemachine_timeout_add` were also moved to `initialise_workers()`.\n\nSo I'm thinking maybe a priority mechanism for the queue, or, maybe just raising an error in the mainloop if some worker job has been queued up before the worker was initialised.", "markup": "markdown", "html": "<p>So it looks like the Pulseblaster's status monitor timeout is added after its worker is configured, and <code>program_manual()</code> is called by <code>DeviceTab.__init__</code> only after <code>initialise_GUI()</code> is called. </p>\n<p>Neither the DAQmx tab or the PulseBlaster use <code>initialise_workers()</code>, they put their worker initialisation in <code>initialise_GUI()</code>. Even if they did use <code>initialise_workers()</code>, it still runs before the first <code>program_manual()</code>.</p>\n<p>So that's why there's no issue so far.</p>\n<p>Many of our devices don't use <code>initialise_workers()</code>, they just put everything in <code>initialise_GUI()</code>. Looks like the only difference is that save data is restored after <code>initialise_GUI()</code> but before <code>initialise_workers()</code>, so if you're using the save data in the initialisation of the worker, but it also needs to be put in the (previously set up) GUI, then you have to do them separately. Maybe I should change the DAQmx class to use <code>initialise_workers()</code> as a better example. Making the PulseBlaster use it would only work if the <code>statemachine_timeout_add</code> were also moved to <code>initialise_workers()</code>.</p>\n<p>So I'm thinking maybe a priority mechanism for the queue, or, maybe just raising an error in the mainloop if some worker job has been queued up before the worker was initialised.</p>", "type": "rendered"}, "created_on": "2018-10-17T00:53:22.672653+00:00", "user": {"display_name": "Chris Billington", "uuid": "{e363c5a9-5075-4656-afb5-88bd6a6dceeb}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D"}, "html": {"href": "https://bitbucket.org/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/9238baf7300c41c0e7294db922899e6ad=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsCB-1.png"}}, "nickname": "cbillington", "type": "user", "account_id": "557058:cbf1bc43-1dc2-477b-9e25-1a8f40fd7ee3"}, "updated_on": "2018-10-17T00:53:22.683164+00:00", "type": "pullrequest_comment", "id": 79313211}, {"links": {"self": {"href": "data/repositories/labscript_suite/blacs/pullrequests/56/comments/79321179.json"}, "html": {"href": "#!/labscript_suite/blacs/pull-requests/56/_/diff#comment-79321179"}}, "parent": {"id": 79313211, "links": {"self": {"href": "data/repositories/labscript_suite/blacs/pullrequests/56/comments/79313211.json"}, "html": {"href": "#!/labscript_suite/blacs/pull-requests/56/_/diff#comment-79313211"}}, "depth": 3}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 56, "links": {"self": {"href": "data/repositories/labscript_suite/blacs/pullrequests/56.json"}, "html": {"href": "#!/labscript_suite/blacs/pull-requests/56"}}, "title": "Defer starting the worker process until _initialise_worker"}, "content": {"raw": "Hmm. I went back through the commit history and it\u2019s not really clear why I made the change. There are a few nearby commits regarding fixing race conditions in the state machine though, and also adding PyQt support \\(on top of the existing PySide support\\). If it is actually necessary, I\u2019m guessing only one or two devices have the problem and the rest are fine, which is why it doesn\u2019t show up in testing.\n\nMaybe the easiest solution is just to make `prepend` a kwarg of `create_worker`. It can default to `False` if you want, as long as we update all of the existing labscript\\_devices classes to explicitly specify `prepend=True`. That way we can slowly remove it as we are able to test each device.", "markup": "markdown", "html": "<p>Hmm. I went back through the commit history and it\u2019s not really clear why I made the change. There are a few nearby commits regarding fixing race conditions in the state machine though, and also adding PyQt support (on top of the existing PySide support). If it is actually necessary, I\u2019m guessing only one or two devices have the problem and the rest are fine, which is why it doesn\u2019t show up in testing.</p>\n<p>Maybe the easiest solution is just to make <code>prepend</code> a kwarg of <code>create_worker</code>. It can default to <code>False</code> if you want, as long as we update all of the existing labscript_devices classes to explicitly specify <code>prepend=True</code>. That way we can slowly remove it as we are able to test each device.</p>", "type": "rendered"}, "created_on": "2018-10-17T03:55:03.783313+00:00", "user": {"display_name": "Philip Starkey", "uuid": "{48af65db-e5fc-459c-a7eb-52eb1f9a5690}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B48af65db-e5fc-459c-a7eb-52eb1f9a5690%7D"}, "html": {"href": "https://bitbucket.org/%7B48af65db-e5fc-459c-a7eb-52eb1f9a5690%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/dc318537facc47ebe1ae98a7aabacecfd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsPS-0.png"}}, "nickname": "pstarkey", "type": "user", "account_id": "557058:52a111e4-40da-4441-9143-417f95f2db97"}, "updated_on": "2018-10-17T03:55:03.797098+00:00", "type": "pullrequest_comment", "id": 79321179}, {"links": {"self": {"href": "data/repositories/labscript_suite/blacs/pullrequests/56/comments/79424081.json"}, "html": {"href": "#!/labscript_suite/blacs/pull-requests/56/_/diff#comment-79424081"}}, "parent": {"id": 79321179, "links": {"self": {"href": "data/repositories/labscript_suite/blacs/pullrequests/56/comments/79321179.json"}, "html": {"href": "#!/labscript_suite/blacs/pull-requests/56/_/diff#comment-79321179"}}, "depth": 4}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 56, "links": {"self": {"href": "data/repositories/labscript_suite/blacs/pullrequests/56.json"}, "html": {"href": "#!/labscript_suite/blacs/pull-requests/56"}}, "title": "Defer starting the worker process until _initialise_worker"}, "content": {"raw": "So that would be a backward incompatible change in BLACS, as even if we added the arguments to BLACS tabs, new BLACS with old labscript_devices would have incorrect behaviour.\n\nWhat I've done now in this PR is to replace the 'prepend' arg with a 'priority' arg. The state queue is kept sorted, and insertions retain sort order (via the \"insort\" function). By prepending to the other state data stored in the queue the priority (lower numbers = higher priority) and the order in which the state was queued up, queue sort order is equal to execution order.\n\nIt's true that this prevents you from using the queue like a stack (other than using ever higher priorities for each added state), but since we don't actually have a use case for this I don't think it's much loss. Higher priority is really what the 'prepend' argument was made for, so the only difference with using priorities as now implemented is that the order isn't backwards if you add a bunch in a row!\n\nI've made it so that `'init'` is `priority=-1`, as well as `'_quit'`, these two were the only uses of `prepend=True`.\n\nSound ok?\n\nTesting doesn't reveal any problems so far, it looks like the 'prepend' argument was not used or exposed anywhere outside of `tab_base_classes.py`.\n\nExtra advantage of these changes: BLACS starts up faster (in the sense of showing the GUI), as it's not starting loads of processes in the GUI thread. It also doesn't hang upon restart.", "markup": "markdown", "html": "<p>So that would be a backward incompatible change in BLACS, as even if we added the arguments to BLACS tabs, new BLACS with old labscript_devices would have incorrect behaviour.</p>\n<p>What I've done now in this PR is to replace the 'prepend' arg with a 'priority' arg. The state queue is kept sorted, and insertions retain sort order (via the \"insort\" function). By prepending to the other state data stored in the queue the priority (lower numbers = higher priority) and the order in which the state was queued up, queue sort order is equal to execution order.</p>\n<p>It's true that this prevents you from using the queue like a stack (other than using ever higher priorities for each added state), but since we don't actually have a use case for this I don't think it's much loss. Higher priority is really what the 'prepend' argument was made for, so the only difference with using priorities as now implemented is that the order isn't backwards if you add a bunch in a row!</p>\n<p>I've made it so that <code>'init'</code> is <code>priority=-1</code>, as well as <code>'_quit'</code>, these two were the only uses of <code>prepend=True</code>.</p>\n<p>Sound ok?</p>\n<p>Testing doesn't reveal any problems so far, it looks like the 'prepend' argument was not used or exposed anywhere outside of <code>tab_base_classes.py</code>.</p>\n<p>Extra advantage of these changes: BLACS starts up faster (in the sense of showing the GUI), as it's not starting loads of processes in the GUI thread. It also doesn't hang upon restart.</p>", "type": "rendered"}, "created_on": "2018-10-17T16:04:01.019812+00:00", "user": {"display_name": "Chris Billington", "uuid": "{e363c5a9-5075-4656-afb5-88bd6a6dceeb}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D"}, "html": {"href": "https://bitbucket.org/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/9238baf7300c41c0e7294db922899e6ad=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsCB-1.png"}}, "nickname": "cbillington", "type": "user", "account_id": "557058:cbf1bc43-1dc2-477b-9e25-1a8f40fd7ee3"}, "updated_on": "2018-10-17T20:19:21.704791+00:00", "type": "pullrequest_comment", "id": 79424081}, {"links": {"self": {"href": "data/repositories/labscript_suite/blacs/pullrequests/56/comments/79451579.json"}, "html": {"href": "#!/labscript_suite/blacs/pull-requests/56/_/diff#comment-79451579"}}, "parent": {"id": 79321179, "links": {"self": {"href": "data/repositories/labscript_suite/blacs/pullrequests/56/comments/79321179.json"}, "html": {"href": "#!/labscript_suite/blacs/pull-requests/56/_/diff#comment-79321179"}}, "depth": 4}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 56, "links": {"self": {"href": "data/repositories/labscript_suite/blacs/pullrequests/56.json"}, "html": {"href": "#!/labscript_suite/blacs/pull-requests/56"}}, "title": "Defer starting the worker process until _initialise_worker"}, "content": {"raw": "Ok, I've done a bit more testing, this revealed one place where I didn't update an index to reflect that there are now more items in the list of things added to the queue. I've now looked at every reference to `self.list_of_states` and don't think I've missed any more.\n\nBeen testing for a little bit on the test setup i'm on (which is if anything a good place to test since I'm restarting and crashing tabs a lot) and haven't turned up any more problems other than another bug in zprocess, which I'll fix and have BLACS in this PR depend on the fix.", "markup": "markdown", "html": "<p>Ok, I've done a bit more testing, this revealed one place where I didn't update an index to reflect that there are now more items in the list of things added to the queue. I've now looked at every reference to <code>self.list_of_states</code> and don't think I've missed any more.</p>\n<p>Been testing for a little bit on the test setup i'm on (which is if anything a good place to test since I'm restarting and crashing tabs a lot) and haven't turned up any more problems other than another bug in zprocess, which I'll fix and have BLACS in this PR depend on the fix.</p>", "type": "rendered"}, "created_on": "2018-10-17T20:27:43.490651+00:00", "user": {"display_name": "Chris Billington", "uuid": "{e363c5a9-5075-4656-afb5-88bd6a6dceeb}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D"}, "html": {"href": "https://bitbucket.org/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/9238baf7300c41c0e7294db922899e6ad=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsCB-1.png"}}, "nickname": "cbillington", "type": "user", "account_id": "557058:cbf1bc43-1dc2-477b-9e25-1a8f40fd7ee3"}, "updated_on": "2018-10-17T20:27:43.504341+00:00", "type": "pullrequest_comment", "id": 79451579}, {"links": {"self": {"href": "data/repositories/labscript_suite/blacs/pullrequests/56/comments/80472918.json"}, "html": {"href": "#!/labscript_suite/blacs/pull-requests/56/_/diff#comment-80472918"}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 56, "links": {"self": {"href": "data/repositories/labscript_suite/blacs/pullrequests/56.json"}, "html": {"href": "#!/labscript_suite/blacs/pull-requests/56"}}, "title": "Defer starting the worker process until _initialise_worker"}, "content": {"raw": "Hey, we have implemented this PR into our branch and encounter the following problem:\n\nWhen using a large amount of worker processes \\(e.g. one PulseBlaster and 5 NI-Cards\\) Blacs fails to initialise all of them. The error message is:\n\n    Fatal exception in main process - Fri Oct 26, 16:48:21 :\n     Traceback (most recent call last):\n      File \"C:\\labscript_suite\\blacs\\tab_base_classes.py\", line 755, in mainloop\n        to_worker, from_worker = worker.start(*worker_args)\n      File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zprocess\\process_tree.py\", line 686, in start\n        response = self.from_child.get(timeout=5)\n      File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zprocess\\process_tree.py\", line 312, in get\n        raise TimeoutError('get() timed out')\n    zprocess.utils.TimeoutError: get() timed out\n    \n    \n\nWe further investigated this and realised the following:\n\n* Hitting restart on any of the failed workers will get them to start normally\n* Some of the workers start at random instances of restarting BLACS.\n* When just using one PulseBlaster and one NI Card, there are no problems\n* all 8 CPU cores go to maximum during worker creation\n\nThis leads us to the conclusion that starting all workers simultaneusly causes problems in combination with the timeout in zprocess of \u201cjust\u201d 5 seconds. Any idea how to resolve this?\n\nLars and Jan ", "markup": "markdown", "html": "<p>Hey, we have implemented this PR into our branch and encounter the following problem:</p>\n<p>When using a large amount of worker processes (e.g. one PulseBlaster and 5 NI-Cards) Blacs fails to initialise all of them. The error message is:</p>\n<div class=\"codehilite\"><pre><span></span>Fatal exception in main process - Fri Oct 26, 16:48:21 :\n Traceback (most recent call last):\n  File &quot;C:\\labscript_suite\\blacs\\tab_base_classes.py&quot;, line 755, in mainloop\n    to_worker, from_worker = worker.start(*worker_args)\n  File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zprocess\\process_tree.py&quot;, line 686, in start\n    response = self.from_child.get(timeout=5)\n  File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zprocess\\process_tree.py&quot;, line 312, in get\n    raise TimeoutError(&#39;get() timed out&#39;)\nzprocess.utils.TimeoutError: get() timed out\n</pre></div>\n\n\n<p>We further investigated this and realised the following:</p>\n<ul>\n<li>Hitting restart on any of the failed workers will get them to start normally</li>\n<li>Some of the workers start at random instances of restarting BLACS.</li>\n<li>When just using one PulseBlaster and one NI Card, there are no problems</li>\n<li>all 8 CPU cores go to maximum during worker creation</li>\n</ul>\n<p>This leads us to the conclusion that starting all workers simultaneusly causes problems in combination with the timeout in zprocess of \u201cjust\u201d 5 seconds. Any idea how to resolve this?</p>\n<p>Lars and Jan </p>", "type": "rendered"}, "created_on": "2018-10-26T15:04:01.090559+00:00", "user": {"display_name": "Lars Kohfahl", "uuid": "{14b59397-cbc0-4d8c-8a4a-fe99fb4d2d4a}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B14b59397-cbc0-4d8c-8a4a-fe99fb4d2d4a%7D"}, "html": {"href": "https://bitbucket.org/%7B14b59397-cbc0-4d8c-8a4a-fe99fb4d2d4a%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/630642264cd55e22515678a3a0489ac7d=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsLK-2.png"}}, "nickname": "lkohfahl", "type": "user", "account_id": "5aafc5d11396802a57aa7f3b"}, "updated_on": "2018-10-26T15:04:01.212419+00:00", "type": "pullrequest_comment", "id": 80472918}, {"links": {"self": {"href": "data/repositories/labscript_suite/blacs/pullrequests/56/comments/80474248.json"}, "html": {"href": "#!/labscript_suite/blacs/pull-requests/56/_/diff#comment-80474248"}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 56, "links": {"self": {"href": "data/repositories/labscript_suite/blacs/pullrequests/56.json"}, "html": {"href": "#!/labscript_suite/blacs/pull-requests/56"}}, "title": "Defer starting the worker process until _initialise_worker"}, "content": {"raw": "Thanks for the report.\n\nSo I think this problems stems from the fact that worker initialisation used to be serialised, i.e. BLACS would not start a worker process until it had completed starting the previous one - including Python interpreter startup and all imports etc in the subprocess, which can take a few seconds. Now, BLACS is starting all the workers at the same time. Although I'm sure this is faster in total, it stands to reason that the time between any one worker starting up and completing its startup will be longer, as windows is doing them all at once and will randomly context switch.\n\nI suspect the solution is just to increase the timeout. I'll check, if this timeout is hard-coded in zprocess, I'll expose it so that BLACS can pass in a higher timeout, say, 30 seconds.\n\nThere is a lot of inefficiency in the worker creation due to us keeping things all the classes in the same files in labscript_devices, leading to a lot of unnecessary imports, but now that there are some changes there that allow things to be split up, worker creation will get faster. If we can avoid importing matplitlib and scipy in the worker processes, then probably you wouldn't see those timeouts. But that's an ongoing task, so for the moment I'll just increase the timeout.", "markup": "markdown", "html": "<p>Thanks for the report.</p>\n<p>So I think this problems stems from the fact that worker initialisation used to be serialised, i.e. BLACS would not start a worker process until it had completed starting the previous one - including Python interpreter startup and all imports etc in the subprocess, which can take a few seconds. Now, BLACS is starting all the workers at the same time. Although I'm sure this is faster in total, it stands to reason that the time between any one worker starting up and completing its startup will be longer, as windows is doing them all at once and will randomly context switch.</p>\n<p>I suspect the solution is just to increase the timeout. I'll check, if this timeout is hard-coded in zprocess, I'll expose it so that BLACS can pass in a higher timeout, say, 30 seconds.</p>\n<p>There is a lot of inefficiency in the worker creation due to us keeping things all the classes in the same files in labscript_devices, leading to a lot of unnecessary imports, but now that there are some changes there that allow things to be split up, worker creation will get faster. If we can avoid importing matplitlib and scipy in the worker processes, then probably you wouldn't see those timeouts. But that's an ongoing task, so for the moment I'll just increase the timeout.</p>", "type": "rendered"}, "created_on": "2018-10-26T15:12:10.537362+00:00", "user": {"display_name": "Chris Billington", "uuid": "{e363c5a9-5075-4656-afb5-88bd6a6dceeb}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D"}, "html": {"href": "https://bitbucket.org/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/9238baf7300c41c0e7294db922899e6ad=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsCB-1.png"}}, "nickname": "cbillington", "type": "user", "account_id": "557058:cbf1bc43-1dc2-477b-9e25-1a8f40fd7ee3"}, "updated_on": "2018-10-26T15:12:10.551920+00:00", "type": "pullrequest_comment", "id": 80474248}, {"links": {"self": {"href": "data/repositories/labscript_suite/blacs/pullrequests/56/comments/80474681.json"}, "html": {"href": "#!/labscript_suite/blacs/pull-requests/56/_/diff#comment-80474681"}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 56, "links": {"self": {"href": "data/repositories/labscript_suite/blacs/pullrequests/56.json"}, "html": {"href": "#!/labscript_suite/blacs/pull-requests/56"}}, "title": "Defer starting the worker process until _initialise_worker"}, "content": {"raw": "I may also change zprocess to use a semaphore to limit the number of simultaneous subprocess creations-in-progress to the number of CPU cores. This will reduce context switching and speed things up a bit, and will make the time for each process to start up less variable.", "markup": "markdown", "html": "<p>I may also change zprocess to use a semaphore to limit the number of simultaneous subprocess creations-in-progress to the number of CPU cores. This will reduce context switching and speed things up a bit, and will make the time for each process to start up less variable.</p>", "type": "rendered"}, "created_on": "2018-10-26T15:14:25.416152+00:00", "user": {"display_name": "Chris Billington", "uuid": "{e363c5a9-5075-4656-afb5-88bd6a6dceeb}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D"}, "html": {"href": "https://bitbucket.org/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/9238baf7300c41c0e7294db922899e6ad=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsCB-1.png"}}, "nickname": "cbillington", "type": "user", "account_id": "557058:cbf1bc43-1dc2-477b-9e25-1a8f40fd7ee3"}, "updated_on": "2018-10-26T15:14:37.366734+00:00", "type": "pullrequest_comment", "id": 80474681}, {"links": {"self": {"href": "data/repositories/labscript_suite/blacs/pullrequests/56/comments/80478655.json"}, "html": {"href": "#!/labscript_suite/blacs/pull-requests/56/_/diff#comment-80478655"}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 56, "links": {"self": {"href": "data/repositories/labscript_suite/blacs/pullrequests/56.json"}, "html": {"href": "#!/labscript_suite/blacs/pull-requests/56"}}, "title": "Defer starting the worker process until _initialise_worker"}, "content": {"raw": "Ok, the timeout issue is hopefully resolved by pull request #60, which requires zprocess 2.9.3.", "markup": "markdown", "html": "<p>Ok, the timeout issue is hopefully resolved by <a href=\"#!/labscript_suite/blacs/pull-requests/60/increase-worker-process-startup-timeout\" rel=\"nofollow\" class=\"ap-connect-link\">pull request #60</a>, which requires zprocess 2.9.3.</p>", "type": "rendered"}, "created_on": "2018-10-26T15:41:35.540555+00:00", "user": {"display_name": "Chris Billington", "uuid": "{e363c5a9-5075-4656-afb5-88bd6a6dceeb}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D"}, "html": {"href": "https://bitbucket.org/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/9238baf7300c41c0e7294db922899e6ad=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsCB-1.png"}}, "nickname": "cbillington", "type": "user", "account_id": "557058:cbf1bc43-1dc2-477b-9e25-1a8f40fd7ee3"}, "updated_on": "2018-10-26T15:41:35.549328+00:00", "type": "pullrequest_comment", "id": 80478655}, {"links": {"self": {"href": "data/repositories/labscript_suite/blacs/pullrequests/56/comments/80480094.json"}, "html": {"href": "#!/labscript_suite/blacs/pull-requests/56/_/diff#comment-80480094"}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 56, "links": {"self": {"href": "data/repositories/labscript_suite/blacs/pullrequests/56.json"}, "html": {"href": "#!/labscript_suite/blacs/pull-requests/56"}}, "title": "Defer starting the worker process until _initialise_worker"}, "content": {"raw": "Works for us. Thank you so much!", "markup": "markdown", "html": "<p>Works for us. Thank you so much!</p>", "type": "rendered"}, "created_on": "2018-10-26T15:53:27.217463+00:00", "user": {"display_name": "Lars Kohfahl", "uuid": "{14b59397-cbc0-4d8c-8a4a-fe99fb4d2d4a}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B14b59397-cbc0-4d8c-8a4a-fe99fb4d2d4a%7D"}, "html": {"href": "https://bitbucket.org/%7B14b59397-cbc0-4d8c-8a4a-fe99fb4d2d4a%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/630642264cd55e22515678a3a0489ac7d=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsLK-2.png"}}, "nickname": "lkohfahl", "type": "user", "account_id": "5aafc5d11396802a57aa7f3b"}, "updated_on": "2018-10-26T15:53:27.338153+00:00", "type": "pullrequest_comment", "id": 80480094}], "page": 1, "size": 11}