{"links": {"self": {"href": "data/repositories/labscript_suite/labscript/issues/28/comments/49715198.json"}, "html": {"href": "#!/labscript_suite/labscript/issues/28#comment-49715198"}}, "issue": {"links": {"self": {"href": "data/repositories/labscript_suite/labscript/issues/28.json"}}, "type": "issue", "id": 28, "repository": {"links": {"self": {"href": "data/repositories/labscript_suite/labscript.json"}, "html": {"href": "#!/labscript_suite/labscript"}, "avatar": {"href": "data/bytebucket.org/ravatar/{48848b08-1db5-463b-bbdc-911beadf8bbf}ts=249917"}}, "type": "repository", "name": "labscript", "full_name": "labscript_suite/labscript", "uuid": "{48848b08-1db5-463b-bbdc-911beadf8bbf}"}, "title": "Remote launching of all Blacs devices"}, "content": {"raw": "Hi Fan,\n\nThanks for all the work! I'll have to look over how what you've done works, or at least the zprocess side of things. You've clearly understood what I was going for there, so it looks like this ought to be compatible with our expectations/plans.\n\nThe use of proxy sockets is interesting - I had thought long and hard about how to route all the traffic for a remote process through a single zmq socket and decided it was basically impossible to do what we wanted in that way, even though it would be preferable for e.g. NAT usage. So I will read over your implementation and see if I was wrong about that, or if there is some crucial functionality I was wanting that can't be done if using a proxy. I forget my reasoning, or why I thought it was impossible, but I'm sure reading your code will remind me.\n\nThe curses interface is a nice touch! I should do that for the other servers like the locking server...\n\nRegarding file I/O, the labscript programs already can run on separate computers, and only exchange filepaths rather than files. The files are expected to be on a network drive, and the filepaths are stripped of the network drive prefix (i.e. drive letter) before sending, with the prefix added by the recipient upon receiving (see labscript_utils.shared_drive). We ensure no two clients have the file open in write mode simultaneously using a network locking server (see labscript_utils.h5_lock and zprocess.locking).\n\nIf configuring all computers to use a shared drive (which may be located on one of them) has any major drawbacks, I might be in favour of including network file I/O in the labscript suite, but so far the shared-drive approach has worked well for us, so I think I don't see a great need for this at the moment. \n\nI can't promise when I'll get a chance to look over this, though I might in the next few days (especially since the US government shutdown means I can't go to the lab). Feel free to make a pull request for the zprocess changes, and we can discuss it further there.  We should get the zprocess implementation solid first since it is the underlying layer for the other bits.\n\nThanks again for contributing!", "markup": "markdown", "html": "<p>Hi Fan,</p>\n<p>Thanks for all the work! I'll have to look over how what you've done works, or at least the zprocess side of things. You've clearly understood what I was going for there, so it looks like this ought to be compatible with our expectations/plans.</p>\n<p>The use of proxy sockets is interesting - I had thought long and hard about how to route all the traffic for a remote process through a single zmq socket and decided it was basically impossible to do what we wanted in that way, even though it would be preferable for e.g. NAT usage. So I will read over your implementation and see if I was wrong about that, or if there is some crucial functionality I was wanting that can't be done if using a proxy. I forget my reasoning, or why I thought it was impossible, but I'm sure reading your code will remind me.</p>\n<p>The curses interface is a nice touch! I should do that for the other servers like the locking server...</p>\n<p>Regarding file I/O, the labscript programs already can run on separate computers, and only exchange filepaths rather than files. The files are expected to be on a network drive, and the filepaths are stripped of the network drive prefix (i.e. drive letter) before sending, with the prefix added by the recipient upon receiving (see labscript_utils.shared_drive). We ensure no two clients have the file open in write mode simultaneously using a network locking server (see labscript_utils.h5_lock and zprocess.locking).</p>\n<p>If configuring all computers to use a shared drive (which may be located on one of them) has any major drawbacks, I might be in favour of including network file I/O in the labscript suite, but so far the shared-drive approach has worked well for us, so I think I don't see a great need for this at the moment. </p>\n<p>I can't promise when I'll get a chance to look over this, though I might in the next few days (especially since the US government shutdown means I can't go to the lab). Feel free to make a pull request for the zprocess changes, and we can discuss it further there.  We should get the zprocess implementation solid first since it is the underlying layer for the other bits.</p>\n<p>Thanks again for contributing!</p>", "type": "rendered"}, "created_on": "2019-01-03T03:16:25.581464+00:00", "user": {"display_name": "Chris Billington", "uuid": "{e363c5a9-5075-4656-afb5-88bd6a6dceeb}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D"}, "html": {"href": "https://bitbucket.org/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/9238baf7300c41c0e7294db922899e6ad=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsCB-1.png"}}, "nickname": "cbillington", "type": "user", "account_id": "557058:cbf1bc43-1dc2-477b-9e25-1a8f40fd7ee3"}, "updated_on": null, "type": "issue_comment", "id": 49715198}