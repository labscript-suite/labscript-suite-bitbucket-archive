{"rendered": {"description": {"raw": "Solve issue #21 by using zlog for file logging.\r\n\r\n`zprocess` has gained a logging server in v2.8.1, called zlog.\r\n\r\n`labscript_utils.setup_logging` now checks for a running zlog server, and starts one if it doesn't exist. It then uses a logging handler that passes log messages to the server instead of writing them to disk. The server uses Python standard library logging handlers to do file rotation, and as it alone is the process interacting with the files, there is no conflict over the files, and no performance penalties of having to coordinate file opening and closing between processes.\r\n\r\nAt instantiation of a `zprocess.zlog.ZMQLoggingHandler`, the zlog client contacts the server to confirm it can write to the log file, and the client raises the error if the server says it can't.\r\n\r\nThen, while the application is running and logging is being done, communication to the server is one-way, with the client sending data to the server only. There is no way for the client to know that the file is still being written to, though if the server goes down and the outgoing zmq queue reaches the high water mark, a warning will be printed.\r\n\r\nAt interpreter shutdown the clients send a 'done' message to the server, and it closes the file once all clients are done.  It also closes the file after 5 seconds if no clients send any log messages (but then just opens it again if log messages subsequently arrive). This is to ensure that the file is still closed if programs do not close cleanly. For example I notice that the logger in BLACS' analysis submission thread doesn't seem to send a 'done ' message. I think this is because it is a daemon thread and is not `join()`ed cleanly when BLACS closes. This should be fixed at some point, but it does not cause problems with logging other than delaying the file being closed by the server for up to 5 seconds.\r\n\r\n\r\nA full description of the zlog protocol can be found [here](https://bitbucket.org/cbillington/zprocess/src/default/zprocess/zlog/__main__.py?at=default&fileviewer=file-view-default#__main__.py-31)\r\n\r\nAt present zlog is configured to only listen on localhost, and this change to labscript_utils assumes one is running on localhost. In the future this can be modified so that remote devices etc can send their logs back to a zlog server on the computer running BLACS so that even a muli-computer instance of BLACS will still have a single log file.", "markup": "markdown", "html": "<p>Solve issue <a href=\"#!/labscript_suite/labscript_utils/issues/21/concurrent-log-handler-causes-unbearable\" rel=\"nofollow\" title=\"Concurrent log handler causes unbearable slowdown\" class=\"ap-connect-link\"><s>#21</s></a> by using zlog for file logging.</p>\n<p><code>zprocess</code> has gained a logging server in v2.8.1, called zlog.</p>\n<p><code>labscript_utils.setup_logging</code> now checks for a running zlog server, and starts one if it doesn't exist. It then uses a logging handler that passes log messages to the server instead of writing them to disk. The server uses Python standard library logging handlers to do file rotation, and as it alone is the process interacting with the files, there is no conflict over the files, and no performance penalties of having to coordinate file opening and closing between processes.</p>\n<p>At instantiation of a <code>zprocess.zlog.ZMQLoggingHandler</code>, the zlog client contacts the server to confirm it can write to the log file, and the client raises the error if the server says it can't.</p>\n<p>Then, while the application is running and logging is being done, communication to the server is one-way, with the client sending data to the server only. There is no way for the client to know that the file is still being written to, though if the server goes down and the outgoing zmq queue reaches the high water mark, a warning will be printed.</p>\n<p>At interpreter shutdown the clients send a 'done' message to the server, and it closes the file once all clients are done.  It also closes the file after 5 seconds if no clients send any log messages (but then just opens it again if log messages subsequently arrive). This is to ensure that the file is still closed if programs do not close cleanly. For example I notice that the logger in BLACS' analysis submission thread doesn't seem to send a 'done ' message. I think this is because it is a daemon thread and is not <code>join()</code>ed cleanly when BLACS closes. This should be fixed at some point, but it does not cause problems with logging other than delaying the file being closed by the server for up to 5 seconds.</p>\n<p>A full description of the zlog protocol can be found <a data-is-external-link=\"true\" href=\"https://bitbucket.org/cbillington/zprocess/src/default/zprocess/zlog/__main__.py?at=default&amp;fileviewer=file-view-default#__main__.py-31\" rel=\"nofollow\">here</a></p>\n<p>At present zlog is configured to only listen on localhost, and this change to labscript_utils assumes one is running on localhost. In the future this can be modified so that remote devices etc can send their logs back to a zlog server on the computer running BLACS so that even a muli-computer instance of BLACS will still have a single log file.</p>", "type": "rendered"}, "title": {"raw": "Use zlog for logging", "markup": "markdown", "html": "<p>Use zlog for logging</p>", "type": "rendered"}}, "type": "pullrequest", "description": "Solve issue #21 by using zlog for file logging.\r\n\r\n`zprocess` has gained a logging server in v2.8.1, called zlog.\r\n\r\n`labscript_utils.setup_logging` now checks for a running zlog server, and starts one if it doesn't exist. It then uses a logging handler that passes log messages to the server instead of writing them to disk. The server uses Python standard library logging handlers to do file rotation, and as it alone is the process interacting with the files, there is no conflict over the files, and no performance penalties of having to coordinate file opening and closing between processes.\r\n\r\nAt instantiation of a `zprocess.zlog.ZMQLoggingHandler`, the zlog client contacts the server to confirm it can write to the log file, and the client raises the error if the server says it can't.\r\n\r\nThen, while the application is running and logging is being done, communication to the server is one-way, with the client sending data to the server only. There is no way for the client to know that the file is still being written to, though if the server goes down and the outgoing zmq queue reaches the high water mark, a warning will be printed.\r\n\r\nAt interpreter shutdown the clients send a 'done' message to the server, and it closes the file once all clients are done.  It also closes the file after 5 seconds if no clients send any log messages (but then just opens it again if log messages subsequently arrive). This is to ensure that the file is still closed if programs do not close cleanly. For example I notice that the logger in BLACS' analysis submission thread doesn't seem to send a 'done ' message. I think this is because it is a daemon thread and is not `join()`ed cleanly when BLACS closes. This should be fixed at some point, but it does not cause problems with logging other than delaying the file being closed by the server for up to 5 seconds.\r\n\r\n\r\nA full description of the zlog protocol can be found [here](https://bitbucket.org/cbillington/zprocess/src/default/zprocess/zlog/__main__.py?at=default&fileviewer=file-view-default#__main__.py-31)\r\n\r\nAt present zlog is configured to only listen on localhost, and this change to labscript_utils assumes one is running on localhost. In the future this can be modified so that remote devices etc can send their logs back to a zlog server on the computer running BLACS so that even a muli-computer instance of BLACS will still have a single log file.", "links": {"decline": {"href": "https://api.bitbucket.org/2.0/repositories/labscript_suite/labscript_utils/pullrequests/56/decline"}, "diffstat": {"href": "https://api.bitbucket.org/2.0/repositories/labscript_suite/labscript_utils/diffstat/labscript_suite/labscript_utils:736203440afa%0D547696e50559?from_pullrequest_id=56"}, "commits": {"href": "data/repositories/labscript_suite/labscript_utils/pullrequests/56/commits.json"}, "self": {"href": "data/repositories/labscript_suite/labscript_utils/pullrequests/56.json"}, "comments": {"href": "data/repositories/labscript_suite/labscript_utils/pullrequests/56/comments_page=1.json"}, "merge": {"href": "https://api.bitbucket.org/2.0/repositories/labscript_suite/labscript_utils/pullrequests/56/merge"}, "html": {"href": "#!/labscript_suite/labscript_utils/pull-requests/56"}, "activity": {"href": "data/repositories/labscript_suite/labscript_utils/pullrequests/56/activity.json"}, "diff": {"href": "https://api.bitbucket.org/2.0/repositories/labscript_suite/labscript_utils/diff/labscript_suite/labscript_utils:736203440afa%0D547696e50559?from_pullrequest_id=56"}, "approve": {"href": "https://api.bitbucket.org/2.0/repositories/labscript_suite/labscript_utils/pullrequests/56/approve"}, "statuses": {"href": "data/repositories/labscript_suite/labscript_utils/pullrequests/56/statuses_page=1.json"}}, "title": "Use zlog for logging", "close_source_branch": true, "reviewers": [{"display_name": "Philip Starkey", "uuid": "{48af65db-e5fc-459c-a7eb-52eb1f9a5690}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B48af65db-e5fc-459c-a7eb-52eb1f9a5690%7D"}, "html": {"href": "https://bitbucket.org/%7B48af65db-e5fc-459c-a7eb-52eb1f9a5690%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/dc318537facc47ebe1ae98a7aabacecfd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsPS-0.png"}}, "nickname": "pstarkey", "type": "user", "account_id": "557058:52a111e4-40da-4441-9143-417f95f2db97"}], "id": 56, "destination": {"commit": {"hash": "547696e50559", "type": "commit", "links": {"self": {"href": "data/repositories/labscript_suite/labscript_utils/commit/547696e50559.json"}, "html": {"href": "#!/labscript_suite/labscript_utils/commits/547696e50559"}}}, "repository": {"links": {"self": {"href": "data/repositories/labscript_suite/labscript_utils.json"}, "html": {"href": "#!/labscript_suite/labscript_utils"}, "avatar": {"href": "data/bytebucket.org/ravatar/{68347210-fb1e-4b58-86c0-bd2c04396e63}ts=249922"}}, "type": "repository", "name": "labscript_utils", "full_name": "labscript_suite/labscript_utils", "uuid": "{68347210-fb1e-4b58-86c0-bd2c04396e63}"}, "branch": {"name": "default"}}, "created_on": "2018-09-21T21:28:37.072274+00:00", "summary": {"raw": "Solve issue #21 by using zlog for file logging.\r\n\r\n`zprocess` has gained a logging server in v2.8.1, called zlog.\r\n\r\n`labscript_utils.setup_logging` now checks for a running zlog server, and starts one if it doesn't exist. It then uses a logging handler that passes log messages to the server instead of writing them to disk. The server uses Python standard library logging handlers to do file rotation, and as it alone is the process interacting with the files, there is no conflict over the files, and no performance penalties of having to coordinate file opening and closing between processes.\r\n\r\nAt instantiation of a `zprocess.zlog.ZMQLoggingHandler`, the zlog client contacts the server to confirm it can write to the log file, and the client raises the error if the server says it can't.\r\n\r\nThen, while the application is running and logging is being done, communication to the server is one-way, with the client sending data to the server only. There is no way for the client to know that the file is still being written to, though if the server goes down and the outgoing zmq queue reaches the high water mark, a warning will be printed.\r\n\r\nAt interpreter shutdown the clients send a 'done' message to the server, and it closes the file once all clients are done.  It also closes the file after 5 seconds if no clients send any log messages (but then just opens it again if log messages subsequently arrive). This is to ensure that the file is still closed if programs do not close cleanly. For example I notice that the logger in BLACS' analysis submission thread doesn't seem to send a 'done ' message. I think this is because it is a daemon thread and is not `join()`ed cleanly when BLACS closes. This should be fixed at some point, but it does not cause problems with logging other than delaying the file being closed by the server for up to 5 seconds.\r\n\r\n\r\nA full description of the zlog protocol can be found [here](https://bitbucket.org/cbillington/zprocess/src/default/zprocess/zlog/__main__.py?at=default&fileviewer=file-view-default#__main__.py-31)\r\n\r\nAt present zlog is configured to only listen on localhost, and this change to labscript_utils assumes one is running on localhost. In the future this can be modified so that remote devices etc can send their logs back to a zlog server on the computer running BLACS so that even a muli-computer instance of BLACS will still have a single log file.", "markup": "markdown", "html": "<p>Solve issue <a href=\"#!/labscript_suite/labscript_utils/issues/21/concurrent-log-handler-causes-unbearable\" rel=\"nofollow\" title=\"Concurrent log handler causes unbearable slowdown\" class=\"ap-connect-link\"><s>#21</s></a> by using zlog for file logging.</p>\n<p><code>zprocess</code> has gained a logging server in v2.8.1, called zlog.</p>\n<p><code>labscript_utils.setup_logging</code> now checks for a running zlog server, and starts one if it doesn't exist. It then uses a logging handler that passes log messages to the server instead of writing them to disk. The server uses Python standard library logging handlers to do file rotation, and as it alone is the process interacting with the files, there is no conflict over the files, and no performance penalties of having to coordinate file opening and closing between processes.</p>\n<p>At instantiation of a <code>zprocess.zlog.ZMQLoggingHandler</code>, the zlog client contacts the server to confirm it can write to the log file, and the client raises the error if the server says it can't.</p>\n<p>Then, while the application is running and logging is being done, communication to the server is one-way, with the client sending data to the server only. There is no way for the client to know that the file is still being written to, though if the server goes down and the outgoing zmq queue reaches the high water mark, a warning will be printed.</p>\n<p>At interpreter shutdown the clients send a 'done' message to the server, and it closes the file once all clients are done.  It also closes the file after 5 seconds if no clients send any log messages (but then just opens it again if log messages subsequently arrive). This is to ensure that the file is still closed if programs do not close cleanly. For example I notice that the logger in BLACS' analysis submission thread doesn't seem to send a 'done ' message. I think this is because it is a daemon thread and is not <code>join()</code>ed cleanly when BLACS closes. This should be fixed at some point, but it does not cause problems with logging other than delaying the file being closed by the server for up to 5 seconds.</p>\n<p>A full description of the zlog protocol can be found <a data-is-external-link=\"true\" href=\"https://bitbucket.org/cbillington/zprocess/src/default/zprocess/zlog/__main__.py?at=default&amp;fileviewer=file-view-default#__main__.py-31\" rel=\"nofollow\">here</a></p>\n<p>At present zlog is configured to only listen on localhost, and this change to labscript_utils assumes one is running on localhost. In the future this can be modified so that remote devices etc can send their logs back to a zlog server on the computer running BLACS so that even a muli-computer instance of BLACS will still have a single log file.</p>", "type": "rendered"}, "source": {"commit": {"hash": "bbf10d6e8024", "type": "commit", "links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/cbillington/labscript_utils/commit/bbf10d6e8024"}, "html": {"href": "#!/cbillington/labscript_utils/commits/bbf10d6e8024"}}}, "repository": {"links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/cbillington/labscript_utils"}, "html": {"href": "#!/cbillington/labscript_utils"}, "avatar": {"href": "data/bytebucket.org/ravatar/{455d0b5d-4852-4e4a-b8b1-f5ba77882210}ts=python"}}, "type": "repository", "name": "labscript_utils", "full_name": "cbillington/labscript_utils", "uuid": "{455d0b5d-4852-4e4a-b8b1-f5ba77882210}"}, "branch": {"name": "use_zlog"}}, "comment_count": 3, "state": "MERGED", "task_count": 0, "participants": [{"role": "PARTICIPANT", "participated_on": "2018-09-26T18:46:08.859109+00:00", "type": "participant", "approved": false, "user": {"display_name": "Chris Billington", "uuid": "{e363c5a9-5075-4656-afb5-88bd6a6dceeb}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D"}, "html": {"href": "https://bitbucket.org/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/9238baf7300c41c0e7294db922899e6ad=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsCB-1.png"}}, "nickname": "cbillington", "type": "user", "account_id": "557058:cbf1bc43-1dc2-477b-9e25-1a8f40fd7ee3"}}, {"role": "REVIEWER", "participated_on": null, "type": "participant", "approved": false, "user": {"display_name": "Philip Starkey", "uuid": "{48af65db-e5fc-459c-a7eb-52eb1f9a5690}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B48af65db-e5fc-459c-a7eb-52eb1f9a5690%7D"}, "html": {"href": "https://bitbucket.org/%7B48af65db-e5fc-459c-a7eb-52eb1f9a5690%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/dc318537facc47ebe1ae98a7aabacecfd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsPS-0.png"}}, "nickname": "pstarkey", "type": "user", "account_id": "557058:52a111e4-40da-4441-9143-417f95f2db97"}}, {"role": "PARTICIPANT", "participated_on": "2018-09-26T18:30:17.875578+00:00", "type": "participant", "approved": false, "user": {"display_name": "Lars Kohfahl", "uuid": "{14b59397-cbc0-4d8c-8a4a-fe99fb4d2d4a}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B14b59397-cbc0-4d8c-8a4a-fe99fb4d2d4a%7D"}, "html": {"href": "https://bitbucket.org/%7B14b59397-cbc0-4d8c-8a4a-fe99fb4d2d4a%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/630642264cd55e22515678a3a0489ac7d=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsLK-2.png"}}, "nickname": "lkohfahl", "type": "user", "account_id": "5aafc5d11396802a57aa7f3b"}}], "reason": "", "updated_on": "2018-10-01T18:57:35.258049+00:00", "author": {"display_name": "Chris Billington", "uuid": "{e363c5a9-5075-4656-afb5-88bd6a6dceeb}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D"}, "html": {"href": "https://bitbucket.org/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/9238baf7300c41c0e7294db922899e6ad=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsCB-1.png"}}, "nickname": "cbillington", "type": "user", "account_id": "557058:cbf1bc43-1dc2-477b-9e25-1a8f40fd7ee3"}, "merge_commit": {"hash": "736203440afa", "type": "commit", "links": {"self": {"href": "data/repositories/labscript_suite/labscript_utils/commit/736203440afa.json"}, "html": {"href": "#!/labscript_suite/labscript_utils/commits/736203440afa"}}}, "closed_by": {"display_name": "Chris Billington", "uuid": "{e363c5a9-5075-4656-afb5-88bd6a6dceeb}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D"}, "html": {"href": "https://bitbucket.org/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/9238baf7300c41c0e7294db922899e6ad=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsCB-1.png"}}, "nickname": "cbillington", "type": "user", "account_id": "557058:cbf1bc43-1dc2-477b-9e25-1a8f40fd7ee3"}}