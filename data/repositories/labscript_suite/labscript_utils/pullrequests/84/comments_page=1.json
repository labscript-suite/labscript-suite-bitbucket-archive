{"pagelen": 100, "values": [{"links": {"self": {"href": "data/repositories/labscript_suite/labscript_utils/pullrequests/84/comments/104165177.json"}, "html": {"href": "#!/labscript_suite/labscript_utils/pull-requests/84/_/diff#comment-104165177"}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 84, "links": {"self": {"href": "data/repositories/labscript_suite/labscript_utils/pullrequests/84.json"}, "html": {"href": "#!/labscript_suite/labscript_utils/pull-requests/84"}}, "title": "Delay handling SIGTERM until HDF5 files are closed"}, "content": {"raw": "This was causing occasional random deadlocks, which were super difficult to find the root cause of. And the deadlocks didn't even have anything to do with signal handling.\n\nTurns out that `h5py.File.__exit__` acquires the h5py global lock (\"`phil`\"), and so it calls our `close()` method with that lock held. For thread-safety in the implementation of the `KillLock`, `KillLock.release()` acquires a different lock, briefly, which means it may be waiting on another thread that currently holds that lock. Said other thread however may choose at any time to do garbage collection, and for deallocating HDF5 objects, this means acquiring the `phil` lock. So that's a deadlock, with tracebacks showing one Python thread hanging within `kill_lock.release()` trying to acquire a lock, and another Python thread hanging on a seemingly innocent line of code inside `kill_lock.acquire()` (whereas it is actually hanging on the garbage collection).\n\nSolution is to define `__exit__` not to acquire `phil`. There is no reason for it to anyway, it is just the h5py devs wrapping almost every method in the lock whether it specifically needs it or not, as far as I can tell.\n\nI will test this on Windows (have already tested pretty well on linux), and will merge Monday June 9th if there are no issues.", "markup": "markdown", "html": "<p>This was causing occasional random deadlocks, which were super difficult to find the root cause of. And the deadlocks didn't even have anything to do with signal handling.</p>\n<p>Turns out that <code>h5py.File.__exit__</code> acquires the h5py global lock (\"<code>phil</code>\"), and so it calls our <code>close()</code> method with that lock held. For thread-safety in the implementation of the <code>KillLock</code>, <code>KillLock.release()</code> acquires a different lock, briefly, which means it may be waiting on another thread that currently holds that lock. Said other thread however may choose at any time to do garbage collection, and for deallocating HDF5 objects, this means acquiring the <code>phil</code> lock. So that's a deadlock, with tracebacks showing one Python thread hanging within <code>kill_lock.release()</code> trying to acquire a lock, and another Python thread hanging on a seemingly innocent line of code inside <code>kill_lock.acquire()</code> (whereas it is actually hanging on the garbage collection).</p>\n<p>Solution is to define <code>__exit__</code> not to acquire <code>phil</code>. There is no reason for it to anyway, it is just the h5py devs wrapping almost every method in the lock whether it specifically needs it or not, as far as I can tell.</p>\n<p>I will test this on Windows (have already tested pretty well on linux), and will merge Monday June 9th if there are no issues.</p>", "type": "rendered"}, "created_on": "2019-05-31T20:50:16.702914+00:00", "user": {"display_name": "Chris Billington", "uuid": "{e363c5a9-5075-4656-afb5-88bd6a6dceeb}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D"}, "html": {"href": "https://bitbucket.org/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/9238baf7300c41c0e7294db922899e6ad=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsCB-1.png"}}, "nickname": "cbillington", "type": "user", "account_id": "557058:cbf1bc43-1dc2-477b-9e25-1a8f40fd7ee3"}, "updated_on": "2019-05-31T20:50:16.713678+00:00", "type": "pullrequest_comment", "id": 104165177}, {"links": {"self": {"href": "data/repositories/labscript_suite/labscript_utils/pullrequests/84/comments/104503776.json"}, "html": {"href": "#!/labscript_suite/labscript_utils/pull-requests/84/_/diff#comment-104503776"}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 84, "links": {"self": {"href": "data/repositories/labscript_suite/labscript_utils/pullrequests/84.json"}, "html": {"href": "#!/labscript_suite/labscript_utils/pull-requests/84"}}, "title": "Delay handling SIGTERM until HDF5 files are closed"}, "content": {"raw": "Testing on Windows has not revealed any issues.", "markup": "markdown", "html": "<p>Testing on Windows has not revealed any issues.</p>", "type": "rendered"}, "created_on": "2019-06-04T18:56:39.305552+00:00", "user": {"display_name": "Chris Billington", "uuid": "{e363c5a9-5075-4656-afb5-88bd6a6dceeb}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D"}, "html": {"href": "https://bitbucket.org/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/9238baf7300c41c0e7294db922899e6ad=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsCB-1.png"}}, "nickname": "cbillington", "type": "user", "account_id": "557058:cbf1bc43-1dc2-477b-9e25-1a8f40fd7ee3"}, "updated_on": "2019-06-04T18:56:39.320039+00:00", "type": "pullrequest_comment", "id": 104503776}, {"links": {"self": {"href": "data/repositories/labscript_suite/labscript_utils/pullrequests/84/comments/105020350.json"}, "html": {"href": "#!/labscript_suite/labscript_utils/pull-requests/84/_/diff#comment-105020350"}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 84, "links": {"self": {"href": "data/repositories/labscript_suite/labscript_utils/pullrequests/84.json"}, "html": {"href": "#!/labscript_suite/labscript_utils/pull-requests/84"}}, "title": "Delay handling SIGTERM until HDF5 files are closed"}, "content": {"raw": "Will delay merging for a bit - our lab has seen a frozen runmanager in the last few days, sounding a lot like the aforementioned deadlock. It's not clear what version was being run at the time, whether it included the fix for the deadlock or not, so I'll wait to see if it recurs. I've hooked up the `faulthandler` module to dump tracebacks of all threads every 60 seconds so that we can diagnose a deadlock if it occurs (so long as the deadlock isn't holding the GIL, which the previous one was not). So I'll give it at least 2 weeks before merging this.", "markup": "markdown", "html": "<p>Will delay merging for a bit - our lab has seen a frozen runmanager in the last few days, sounding a lot like the aforementioned deadlock. It's not clear what version was being run at the time, whether it included the fix for the deadlock or not, so I'll wait to see if it recurs. I've hooked up the <code>faulthandler</code> module to dump tracebacks of all threads every 60 seconds so that we can diagnose a deadlock if it occurs (so long as the deadlock isn't holding the GIL, which the previous one was not). So I'll give it at least 2 weeks before merging this.</p>", "type": "rendered"}, "created_on": "2019-06-09T16:31:25.094058+00:00", "user": {"display_name": "Chris Billington", "uuid": "{e363c5a9-5075-4656-afb5-88bd6a6dceeb}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D"}, "html": {"href": "https://bitbucket.org/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/9238baf7300c41c0e7294db922899e6ad=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsCB-1.png"}}, "nickname": "cbillington", "type": "user", "account_id": "557058:cbf1bc43-1dc2-477b-9e25-1a8f40fd7ee3"}, "updated_on": "2019-06-09T16:31:25.100274+00:00", "type": "pullrequest_comment", "id": 105020350}, {"links": {"self": {"href": "data/repositories/labscript_suite/labscript_utils/pullrequests/84/comments/105849898.json"}, "html": {"href": "#!/labscript_suite/labscript_utils/pull-requests/84/_/diff#comment-105849898"}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 84, "links": {"self": {"href": "data/repositories/labscript_suite/labscript_utils/pullrequests/84.json"}, "html": {"href": "#!/labscript_suite/labscript_utils/pull-requests/84"}}, "title": "Delay handling SIGTERM until HDF5 files are closed"}, "content": {"raw": "We haven't seen any more runmanager crashes, and we've been hammering runmanger pretty hard with optimisation setting globals and compiling shots with `runmanager.remote`, so I suspect the version of runmanager that had the crash didn't have the fix for the deadlock mentioned above. So I'm merging this.", "markup": "markdown", "html": "<p>We haven't seen any more runmanager crashes, and we've been hammering runmanger pretty hard with optimisation setting globals and compiling shots with <code>runmanager.remote</code>, so I suspect the version of runmanager that had the crash didn't have the fix for the deadlock mentioned above. So I'm merging this.</p>", "type": "rendered"}, "created_on": "2019-06-15T21:21:33.952691+00:00", "user": {"display_name": "Chris Billington", "uuid": "{e363c5a9-5075-4656-afb5-88bd6a6dceeb}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D"}, "html": {"href": "https://bitbucket.org/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/9238baf7300c41c0e7294db922899e6ad=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsCB-1.png"}}, "nickname": "cbillington", "type": "user", "account_id": "557058:cbf1bc43-1dc2-477b-9e25-1a8f40fd7ee3"}, "updated_on": "2019-06-15T21:21:33.962360+00:00", "type": "pullrequest_comment", "id": 105849898}], "page": 1, "size": 4}