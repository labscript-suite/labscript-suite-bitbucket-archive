{"links": {"self": {"href": "data/repositories/labscript_suite/lyse/issues/29/comments/38283641.json"}, "html": {"href": "#!/labscript_suite/lyse/issues/29#comment-38283641"}}, "issue": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse/issues/29.json"}}, "type": "issue", "id": 29, "repository": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse.json"}, "html": {"href": "#!/labscript_suite/lyse"}, "avatar": {"href": "data/bytebucket.org/ravatar/{55eebdfe-43d1-4ae8-9049-50c55b295397}ts=249921"}}, "type": "repository", "name": "lyse", "full_name": "labscript_suite/lyse", "uuid": "{55eebdfe-43d1-4ae8-9049-50c55b295397}"}, "title": "speed up shot loading"}, "content": {"raw": "Parallelising HDF5 file reading with threads will not achieve anything as h5py holds the python GIL for all operations. The comment on line 1623 `# We open the HDF5 files here outside the GUI thread so as not to hang the GUI:` is not (yet) true. [H5py plan to remove the GIL holding](https://groups.google.com/forum/#!topic/h5py/kucoDPOpe2E) at some point, but have not yet to my knowledge. They also will most likely replace it with their own lock, which means the GUI won't hang, but parallel hdf5 read operations likely won't yield much either. Furthermore, whilst the files are open, I suspect the bottleneck is likely to be disk and network, not python code execution speed, meaning parallelisation won't yield much unless the files are on different disks or something. So even if the reading was parallelised with multiple *processes* rather than threads, I think it would still not result in much speedup.\n\nAs you've discovered though, there are lots of gains to be had outside of file reading. We are not particularly efficient with many of our dataframe operations, and I'm sure there are gains to be had in the interaction with the Qt model as well.\n\nI'll have a look at your above suggestions!", "markup": "markdown", "html": "<p>Parallelising HDF5 file reading with threads will not achieve anything as h5py holds the python GIL for all operations. The comment on line 1623 <code># We open the HDF5 files here outside the GUI thread so as not to hang the GUI:</code> is not (yet) true. <a data-is-external-link=\"true\" href=\"https://groups.google.com/forum/#!topic/h5py/kucoDPOpe2E\" rel=\"nofollow\">H5py plan to remove the GIL holding</a> at some point, but have not yet to my knowledge. They also will most likely replace it with their own lock, which means the GUI won't hang, but parallel hdf5 read operations likely won't yield much either. Furthermore, whilst the files are open, I suspect the bottleneck is likely to be disk and network, not python code execution speed, meaning parallelisation won't yield much unless the files are on different disks or something. So even if the reading was parallelised with multiple <em>processes</em> rather than threads, I think it would still not result in much speedup.</p>\n<p>As you've discovered though, there are lots of gains to be had outside of file reading. We are not particularly efficient with many of our dataframe operations, and I'm sure there are gains to be had in the interaction with the Qt model as well.</p>\n<p>I'll have a look at your above suggestions!</p>", "type": "rendered"}, "created_on": "2017-07-17T16:29:32.854889+00:00", "user": {"display_name": "Chris Billington", "uuid": "{e363c5a9-5075-4656-afb5-88bd6a6dceeb}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D"}, "html": {"href": "https://bitbucket.org/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/9238baf7300c41c0e7294db922899e6ad=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsCB-1.png"}}, "nickname": "cbillington", "type": "user", "account_id": "557058:cbf1bc43-1dc2-477b-9e25-1a8f40fd7ee3"}, "updated_on": null, "type": "issue_comment", "id": 38283641}