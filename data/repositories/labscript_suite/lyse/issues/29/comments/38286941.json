{"links": {"self": {"href": "data/repositories/labscript_suite/lyse/issues/29/comments/38286941.json"}, "html": {"href": "#!/labscript_suite/lyse/issues/29#comment-38286941"}}, "issue": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse/issues/29.json"}}, "type": "issue", "id": 29, "repository": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse.json"}, "html": {"href": "#!/labscript_suite/lyse"}, "avatar": {"href": "data/bytebucket.org/ravatar/{55eebdfe-43d1-4ae8-9049-50c55b295397}ts=249921"}}, "type": "repository", "name": "lyse", "full_name": "labscript_suite/lyse", "uuid": "{55eebdfe-43d1-4ae8-9049-50c55b295397}"}, "title": "speed up shot loading"}, "content": {"raw": "Will definitely do diffs in the future (or create a branch on my fork).\n\nWell update_row is current called in 2 places add_files and in the analysis loop. So I don't think this is too bad.\nA check is probably a good idea will add that.\n\nYes as I wrote allready I'm importing/exporting 2000 shot dateframes as a whole(one file) and see a speed increase of a factor 2 there. And update_row is the only thing I'm editing currently(besides add_files) so I guess update_row becomes 2 times faster as a whole. I'm not quite sure how this affects loading 2000 files but would assume a speedup as well but smaller.\n\nPandas supports indexing(https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_index.html) and as I've read this also speeds up searches. You then reference rows in loc by that index like df.loc[filepath, column]. But i'm not that great with pandas so thats for someone else to do.\n\nThe main slowness I think here is the string comparison though.\n\nYes the set is unordered and well order seems to be better than chaos so thats why I'm later using a list.\n\nWith the other changes made I'm trying to reduce the amount of loops as they were not really needed.\n\nYes I'm waiting on the Update Dataframe in particular as they both touch the update_row function and I don't really want to end up with conflicts again. Also I'm sure there is more speed that can be gained so I'm not in a rush.\n\nThe incoming buffer I looked at butnothing really sticked out. I already wrote a workaround with the dataframe files that can be exported and imported. Is this something that could be of interest to others? Or is the goal to rather improve loading individual files?", "markup": "markdown", "html": "<p>Will definitely do diffs in the future (or create a branch on my fork).</p>\n<p>Well update_row is current called in 2 places add_files and in the analysis loop. So I don't think this is too bad.\nA check is probably a good idea will add that.</p>\n<p>Yes as I wrote allready I'm importing/exporting 2000 shot dateframes as a whole(one file) and see a speed increase of a factor 2 there. And update_row is the only thing I'm editing currently(besides add_files) so I guess update_row becomes 2 times faster as a whole. I'm not quite sure how this affects loading 2000 files but would assume a speedup as well but smaller.</p>\n<p>Pandas supports indexing(<a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_index.html\" rel=\"nofollow\" class=\"ap-connect-link\">https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_index.html</a>) and as I've read this also speeds up searches. You then reference rows in loc by that index like df.loc[filepath, column]. But i'm not that great with pandas so thats for someone else to do.</p>\n<p>The main slowness I think here is the string comparison though.</p>\n<p>Yes the set is unordered and well order seems to be better than chaos so thats why I'm later using a list.</p>\n<p>With the other changes made I'm trying to reduce the amount of loops as they were not really needed.</p>\n<p>Yes I'm waiting on the Update Dataframe in particular as they both touch the update_row function and I don't really want to end up with conflicts again. Also I'm sure there is more speed that can be gained so I'm not in a rush.</p>\n<p>The incoming buffer I looked at butnothing really sticked out. I already wrote a workaround with the dataframe files that can be exported and imported. Is this something that could be of interest to others? Or is the goal to rather improve loading individual files?</p>", "type": "rendered"}, "created_on": "2017-07-17T19:07:40.887662+00:00", "user": {"display_name": "Jan Werkmann", "uuid": "{44c4905c-2b90-4045-a5f1-652b8e228626}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B44c4905c-2b90-4045-a5f1-652b8e228626%7D"}, "html": {"href": "https://bitbucket.org/%7B44c4905c-2b90-4045-a5f1-652b8e228626%7D/"}, "avatar": {"href": "https://avatar-management--avatars.us-west-2.prod.public.atl-paas.net/557058:a70cc9cf-684e-4849-a61a-9ade4d7218b5/07e5095a-4741-4dc0-a462-9c7d455f961d/128"}}, "nickname": "PhyNerd", "type": "user", "account_id": "557058:a70cc9cf-684e-4849-a61a-9ade4d7218b5"}, "updated_on": null, "type": "issue_comment", "id": 38286941}