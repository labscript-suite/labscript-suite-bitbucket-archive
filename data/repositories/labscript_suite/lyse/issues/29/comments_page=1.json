{"pagelen": 100, "values": [{"links": {"self": {"href": "data/repositories/labscript_suite/lyse/issues/29/comments/38191905.json"}, "html": {"href": "#!/labscript_suite/lyse/issues/29#comment-38191905"}}, "issue": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse/issues/29.json"}}, "type": "issue", "id": 29, "repository": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse.json"}, "html": {"href": "#!/labscript_suite/lyse"}, "avatar": {"href": "data/bytebucket.org/ravatar/{55eebdfe-43d1-4ae8-9049-50c55b295397}ts=249921"}}, "type": "repository", "name": "lyse", "full_name": "labscript_suite/lyse", "uuid": "{55eebdfe-43d1-4ae8-9049-50c55b295397}"}, "title": "speed up shot loading"}, "content": {"raw": "Ok so I did some digging in the code and I was able to speed up loading files by a factor of 3(!!!) by changing just one line. In update_row() I changed\n```\n#!python\n\ndataframe_row = dict(self.dataframe.iloc[df_row_index])\n```\n\n to\n\n```\n#!python\n\ndataframe_row = self.dataframe.iloc[df_row_index].to_dict()\n```\nI don't know why this help but at least for my test machines it does.", "markup": "markdown", "html": "<p>Ok so I did some digging in the code and I was able to speed up loading files by a factor of 3(!!!) by changing just one line. In update_row() I changed</p>\n<div class=\"codehilite language-python\"><pre><span></span><span class=\"n\">dataframe_row</span> <span class=\"o\">=</span> <span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">iloc</span><span class=\"p\">[</span><span class=\"n\">df_row_index</span><span class=\"p\">])</span>\n</pre></div>\n\n\n<p>to</p>\n<div class=\"codehilite language-python\"><pre><span></span><span class=\"n\">dataframe_row</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">iloc</span><span class=\"p\">[</span><span class=\"n\">df_row_index</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">to_dict</span><span class=\"p\">()</span>\n</pre></div>\n\n\n<p>I don't know why this help but at least for my test machines it does.</p>", "type": "rendered"}, "created_on": "2017-07-12T20:53:32.194763+00:00", "user": {"display_name": "Jan Werkmann", "uuid": "{44c4905c-2b90-4045-a5f1-652b8e228626}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B44c4905c-2b90-4045-a5f1-652b8e228626%7D"}, "html": {"href": "https://bitbucket.org/%7B44c4905c-2b90-4045-a5f1-652b8e228626%7D/"}, "avatar": {"href": "https://avatar-management--avatars.us-west-2.prod.public.atl-paas.net/557058:a70cc9cf-684e-4849-a61a-9ade4d7218b5/07e5095a-4741-4dc0-a462-9c7d455f961d/128"}}, "nickname": "PhyNerd", "type": "user", "account_id": "557058:a70cc9cf-684e-4849-a61a-9ade4d7218b5"}, "updated_on": null, "type": "issue_comment", "id": 38191905}, {"links": {"self": {"href": "data/repositories/labscript_suite/lyse/issues/29/comments/38244608.json"}, "html": {"href": "#!/labscript_suite/lyse/issues/29#comment-38244608"}}, "issue": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse/issues/29.json"}}, "type": "issue", "id": 29, "repository": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse.json"}, "html": {"href": "#!/labscript_suite/lyse"}, "avatar": {"href": "data/bytebucket.org/ravatar/{55eebdfe-43d1-4ae8-9049-50c55b295397}ts=249921"}}, "type": "repository", "name": "lyse", "full_name": "labscript_suite/lyse", "uuid": "{55eebdfe-43d1-4ae8-9049-50c55b295397}"}, "title": "speed up shot loading"}, "content": {"raw": null, "markup": "markdown", "html": "", "type": "rendered"}, "created_on": "2017-07-14T19:05:59.481837+00:00", "user": {"display_name": "Jan Werkmann", "uuid": "{44c4905c-2b90-4045-a5f1-652b8e228626}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B44c4905c-2b90-4045-a5f1-652b8e228626%7D"}, "html": {"href": "https://bitbucket.org/%7B44c4905c-2b90-4045-a5f1-652b8e228626%7D/"}, "avatar": {"href": "https://avatar-management--avatars.us-west-2.prod.public.atl-paas.net/557058:a70cc9cf-684e-4849-a61a-9ade4d7218b5/07e5095a-4741-4dc0-a462-9c7d455f961d/128"}}, "nickname": "PhyNerd", "type": "user", "account_id": "557058:a70cc9cf-684e-4849-a61a-9ade4d7218b5"}, "updated_on": null, "type": "issue_comment", "id": 38244608}, {"links": {"self": {"href": "data/repositories/labscript_suite/lyse/issues/29/comments/38244677.json"}, "html": {"href": "#!/labscript_suite/lyse/issues/29#comment-38244677"}}, "issue": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse/issues/29.json"}}, "type": "issue", "id": 29, "repository": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse.json"}, "html": {"href": "#!/labscript_suite/lyse"}, "avatar": {"href": "data/bytebucket.org/ravatar/{55eebdfe-43d1-4ae8-9049-50c55b295397}ts=249921"}}, "type": "repository", "name": "lyse", "full_name": "labscript_suite/lyse", "uuid": "{55eebdfe-43d1-4ae8-9049-50c55b295397}"}, "title": "speed up shot loading"}, "content": {"raw": "Ok another thing that gives me a factor 2 speed improvement is instead of using the filepath to reference the line that needs updating(in update_row) rather use the dataframe index as this is 2 times faster. The filepath can then be retrieved from the dataframe row.\n\n\n```\n#!python\n\n    @inmain_decorator()\n    def update_row(self, filepath=None, dataframe_already_updated=False, status_percent=None, new_row_data=None, updated_row_data=None, df_row_index=None):\n        \"\"\"\"Updates a row in the dataframe and Qt model\n        to the data in the HDF5 file for that shot. Also sets the percent done, if specified\"\"\"\n        # Update the row in the dataframe first:\n        if df_row_index is None:\n            if filepath is None:\n                raise ValueError('Eigther df_row_index of filepath must be provided!')\n            df_row_index = np.where(self.dataframe['filepath'].values == filepath)\n            try:\n                df_row_index = df_row_index[0][0]\n            except IndexError:\n                # Row has been deleted, nothing to do here:\n                return\n```\nand\n\n```\n#!python\n\n        # Update the data in the Qt model:\n        dataframe_row = self.dataframe.iloc[df_row_index].to_dict()\n        if filepath is None:\n            filepath = dataframe_row['filepath', '']\n        model_row_number = self.get_model_row_by_filepath(filepath)\n```\n\n\nA small improvement in speed was also gained when reordering and rewriting add_files:\n\n```\n#!python\n\n    @inmain_decorator()\n    def add_files(self, filepaths, new_row_data):\n        \"\"\"Add files to the dataframe model. New_row_data should be a\n        dataframe containing the new rows.\"\"\"\n        filepaths = set(filepaths)\n        duplicates = set(self.dataframe['filepath'].values)-(set(self.dataframe['filepath'].values)-filepaths)\n        for filepath in duplicates:\n            app.output_box.output('Warning: Ignoring duplicate shot %s\\n' % filepath, red=True)\n            if new_row_data is not None:\n                df_row_index = np.where(new_row_data['filepath'].values == filepath)\n                new_row_data = new_row_data.drop(df_row_index[0])\n                new_row_data.index = pandas.Index(range(len(new_row_data)))\n\n        df_len = len(self.dataframe)\n        self.dataframe = concat_with_padding(self.dataframe, new_row_data)\n        self.update_column_levels()\n        filepaths = new_row_data[\"filepath\"].tolist()\n        for i, filepath in enumerate(filepaths):\n            # Add the new rows to the model:\n            self._model.appendRow(self.new_row(filepath))\n            vert_header_item = QtGui.QStandardItem('...loading...')\n            self._model.setVerticalHeaderItem(self._model.rowCount() - 1, vert_header_item)\n            self._view.resizeRowToContents(self._model.rowCount() - 1)\n            self.update_row(dataframe_already_updated=True, df_row_index=i+df_len)\n        self.renumber_rows()\n```\n\nTo test speed i'm using a feature that I'm currently writing to export and import dataframes. I reload the same dataframe of 2000 shots everytime(from one file).\nAt the beginning this took 45 s with all the changes I made I'm down to about 6 s. \nI'm trying to reach 2 s maybe 1 s.\nAny more suggestions for improvement?\nIf this feature is of interest I could also provide this to the main branch.", "markup": "markdown", "html": "<p>Ok another thing that gives me a factor 2 speed improvement is instead of using the filepath to reference the line that needs updating(in update_row) rather use the dataframe index as this is 2 times faster. The filepath can then be retrieved from the dataframe row.</p>\n<div class=\"codehilite language-python\"><pre><span></span>    <span class=\"nd\">@inmain_decorator</span><span class=\"p\">()</span>\n    <span class=\"k\">def</span> <span class=\"nf\">update_row</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">filepath</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span> <span class=\"n\">dataframe_already_updated</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">,</span> <span class=\"n\">status_percent</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span> <span class=\"n\">new_row_data</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span> <span class=\"n\">updated_row_data</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span> <span class=\"n\">df_row_index</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">):</span>\n        <span class=\"sd\">&quot;&quot;&quot;&quot;Updates a row in the dataframe and Qt model</span>\n<span class=\"sd\">        to the data in the HDF5 file for that shot. Also sets the percent done, if specified&quot;&quot;&quot;</span>\n        <span class=\"c1\"># Update the row in the dataframe first:</span>\n        <span class=\"k\">if</span> <span class=\"n\">df_row_index</span> <span class=\"ow\">is</span> <span class=\"bp\">None</span><span class=\"p\">:</span>\n            <span class=\"k\">if</span> <span class=\"n\">filepath</span> <span class=\"ow\">is</span> <span class=\"bp\">None</span><span class=\"p\">:</span>\n                <span class=\"k\">raise</span> <span class=\"ne\">ValueError</span><span class=\"p\">(</span><span class=\"s1\">&#39;Eigther df_row_index of filepath must be provided!&#39;</span><span class=\"p\">)</span>\n            <span class=\"n\">df_row_index</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"p\">[</span><span class=\"s1\">&#39;filepath&#39;</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">values</span> <span class=\"o\">==</span> <span class=\"n\">filepath</span><span class=\"p\">)</span>\n            <span class=\"k\">try</span><span class=\"p\">:</span>\n                <span class=\"n\">df_row_index</span> <span class=\"o\">=</span> <span class=\"n\">df_row_index</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n            <span class=\"k\">except</span> <span class=\"ne\">IndexError</span><span class=\"p\">:</span>\n                <span class=\"c1\"># Row has been deleted, nothing to do here:</span>\n                <span class=\"k\">return</span>\n</pre></div>\n\n\n<p>and</p>\n<div class=\"codehilite language-python\"><pre><span></span>        <span class=\"c1\"># Update the data in the Qt model:</span>\n        <span class=\"n\">dataframe_row</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">iloc</span><span class=\"p\">[</span><span class=\"n\">df_row_index</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">to_dict</span><span class=\"p\">()</span>\n        <span class=\"k\">if</span> <span class=\"n\">filepath</span> <span class=\"ow\">is</span> <span class=\"bp\">None</span><span class=\"p\">:</span>\n            <span class=\"n\">filepath</span> <span class=\"o\">=</span> <span class=\"n\">dataframe_row</span><span class=\"p\">[</span><span class=\"s1\">&#39;filepath&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;&#39;</span><span class=\"p\">]</span>\n        <span class=\"n\">model_row_number</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">get_model_row_by_filepath</span><span class=\"p\">(</span><span class=\"n\">filepath</span><span class=\"p\">)</span>\n</pre></div>\n\n\n<p>A small improvement in speed was also gained when reordering and rewriting add_files:</p>\n<div class=\"codehilite language-python\"><pre><span></span>    <span class=\"nd\">@inmain_decorator</span><span class=\"p\">()</span>\n    <span class=\"k\">def</span> <span class=\"nf\">add_files</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">filepaths</span><span class=\"p\">,</span> <span class=\"n\">new_row_data</span><span class=\"p\">):</span>\n        <span class=\"sd\">&quot;&quot;&quot;Add files to the dataframe model. New_row_data should be a</span>\n<span class=\"sd\">        dataframe containing the new rows.&quot;&quot;&quot;</span>\n        <span class=\"n\">filepaths</span> <span class=\"o\">=</span> <span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">filepaths</span><span class=\"p\">)</span>\n        <span class=\"n\">duplicates</span> <span class=\"o\">=</span> <span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"p\">[</span><span class=\"s1\">&#39;filepath&#39;</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">values</span><span class=\"p\">)</span><span class=\"o\">-</span><span class=\"p\">(</span><span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"p\">[</span><span class=\"s1\">&#39;filepath&#39;</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">values</span><span class=\"p\">)</span><span class=\"o\">-</span><span class=\"n\">filepaths</span><span class=\"p\">)</span>\n        <span class=\"k\">for</span> <span class=\"n\">filepath</span> <span class=\"ow\">in</span> <span class=\"n\">duplicates</span><span class=\"p\">:</span>\n            <span class=\"n\">app</span><span class=\"o\">.</span><span class=\"n\">output_box</span><span class=\"o\">.</span><span class=\"n\">output</span><span class=\"p\">(</span><span class=\"s1\">&#39;Warning: Ignoring duplicate shot </span><span class=\"si\">%s</span><span class=\"se\">\\n</span><span class=\"s1\">&#39;</span> <span class=\"o\">%</span> <span class=\"n\">filepath</span><span class=\"p\">,</span> <span class=\"n\">red</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n            <span class=\"k\">if</span> <span class=\"n\">new_row_data</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"bp\">None</span><span class=\"p\">:</span>\n                <span class=\"n\">df_row_index</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">new_row_data</span><span class=\"p\">[</span><span class=\"s1\">&#39;filepath&#39;</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">values</span> <span class=\"o\">==</span> <span class=\"n\">filepath</span><span class=\"p\">)</span>\n                <span class=\"n\">new_row_data</span> <span class=\"o\">=</span> <span class=\"n\">new_row_data</span><span class=\"o\">.</span><span class=\"n\">drop</span><span class=\"p\">(</span><span class=\"n\">df_row_index</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span>\n                <span class=\"n\">new_row_data</span><span class=\"o\">.</span><span class=\"n\">index</span> <span class=\"o\">=</span> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">Index</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">new_row_data</span><span class=\"p\">)))</span>\n\n        <span class=\"n\">df_len</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">dataframe</span> <span class=\"o\">=</span> <span class=\"n\">concat_with_padding</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"p\">,</span> <span class=\"n\">new_row_data</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">update_column_levels</span><span class=\"p\">()</span>\n        <span class=\"n\">filepaths</span> <span class=\"o\">=</span> <span class=\"n\">new_row_data</span><span class=\"p\">[</span><span class=\"s2\">&quot;filepath&quot;</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">tolist</span><span class=\"p\">()</span>\n        <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">filepath</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">filepaths</span><span class=\"p\">):</span>\n            <span class=\"c1\"># Add the new rows to the model:</span>\n            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_model</span><span class=\"o\">.</span><span class=\"n\">appendRow</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">new_row</span><span class=\"p\">(</span><span class=\"n\">filepath</span><span class=\"p\">))</span>\n            <span class=\"n\">vert_header_item</span> <span class=\"o\">=</span> <span class=\"n\">QtGui</span><span class=\"o\">.</span><span class=\"n\">QStandardItem</span><span class=\"p\">(</span><span class=\"s1\">&#39;...loading...&#39;</span><span class=\"p\">)</span>\n            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_model</span><span class=\"o\">.</span><span class=\"n\">setVerticalHeaderItem</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_model</span><span class=\"o\">.</span><span class=\"n\">rowCount</span><span class=\"p\">()</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">vert_header_item</span><span class=\"p\">)</span>\n            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_view</span><span class=\"o\">.</span><span class=\"n\">resizeRowToContents</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_model</span><span class=\"o\">.</span><span class=\"n\">rowCount</span><span class=\"p\">()</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">update_row</span><span class=\"p\">(</span><span class=\"n\">dataframe_already_updated</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"n\">df_row_index</span><span class=\"o\">=</span><span class=\"n\">i</span><span class=\"o\">+</span><span class=\"n\">df_len</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">renumber_rows</span><span class=\"p\">()</span>\n</pre></div>\n\n\n<p>To test speed i'm using a feature that I'm currently writing to export and import dataframes. I reload the same dataframe of 2000 shots everytime(from one file).\nAt the beginning this took 45 s with all the changes I made I'm down to about 6 s. \nI'm trying to reach 2 s maybe 1 s.\nAny more suggestions for improvement?\nIf this feature is of interest I could also provide this to the main branch.</p>", "type": "rendered"}, "created_on": "2017-07-14T19:10:12.699874+00:00", "user": {"display_name": "Jan Werkmann", "uuid": "{44c4905c-2b90-4045-a5f1-652b8e228626}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B44c4905c-2b90-4045-a5f1-652b8e228626%7D"}, "html": {"href": "https://bitbucket.org/%7B44c4905c-2b90-4045-a5f1-652b8e228626%7D/"}, "avatar": {"href": "https://avatar-management--avatars.us-west-2.prod.public.atl-paas.net/557058:a70cc9cf-684e-4849-a61a-9ade4d7218b5/07e5095a-4741-4dc0-a462-9c7d455f961d/128"}}, "nickname": "PhyNerd", "type": "user", "account_id": "557058:a70cc9cf-684e-4849-a61a-9ade4d7218b5"}, "updated_on": "2017-07-15T12:58:42.294268+00:00", "type": "issue_comment", "id": 38244677}, {"links": {"self": {"href": "data/repositories/labscript_suite/lyse/issues/29/comments/38283641.json"}, "html": {"href": "#!/labscript_suite/lyse/issues/29#comment-38283641"}}, "issue": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse/issues/29.json"}}, "type": "issue", "id": 29, "repository": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse.json"}, "html": {"href": "#!/labscript_suite/lyse"}, "avatar": {"href": "data/bytebucket.org/ravatar/{55eebdfe-43d1-4ae8-9049-50c55b295397}ts=249921"}}, "type": "repository", "name": "lyse", "full_name": "labscript_suite/lyse", "uuid": "{55eebdfe-43d1-4ae8-9049-50c55b295397}"}, "title": "speed up shot loading"}, "content": {"raw": "Parallelising HDF5 file reading with threads will not achieve anything as h5py holds the python GIL for all operations. The comment on line 1623 `# We open the HDF5 files here outside the GUI thread so as not to hang the GUI:` is not (yet) true. [H5py plan to remove the GIL holding](https://groups.google.com/forum/#!topic/h5py/kucoDPOpe2E) at some point, but have not yet to my knowledge. They also will most likely replace it with their own lock, which means the GUI won't hang, but parallel hdf5 read operations likely won't yield much either. Furthermore, whilst the files are open, I suspect the bottleneck is likely to be disk and network, not python code execution speed, meaning parallelisation won't yield much unless the files are on different disks or something. So even if the reading was parallelised with multiple *processes* rather than threads, I think it would still not result in much speedup.\n\nAs you've discovered though, there are lots of gains to be had outside of file reading. We are not particularly efficient with many of our dataframe operations, and I'm sure there are gains to be had in the interaction with the Qt model as well.\n\nI'll have a look at your above suggestions!", "markup": "markdown", "html": "<p>Parallelising HDF5 file reading with threads will not achieve anything as h5py holds the python GIL for all operations. The comment on line 1623 <code># We open the HDF5 files here outside the GUI thread so as not to hang the GUI:</code> is not (yet) true. <a data-is-external-link=\"true\" href=\"https://groups.google.com/forum/#!topic/h5py/kucoDPOpe2E\" rel=\"nofollow\">H5py plan to remove the GIL holding</a> at some point, but have not yet to my knowledge. They also will most likely replace it with their own lock, which means the GUI won't hang, but parallel hdf5 read operations likely won't yield much either. Furthermore, whilst the files are open, I suspect the bottleneck is likely to be disk and network, not python code execution speed, meaning parallelisation won't yield much unless the files are on different disks or something. So even if the reading was parallelised with multiple <em>processes</em> rather than threads, I think it would still not result in much speedup.</p>\n<p>As you've discovered though, there are lots of gains to be had outside of file reading. We are not particularly efficient with many of our dataframe operations, and I'm sure there are gains to be had in the interaction with the Qt model as well.</p>\n<p>I'll have a look at your above suggestions!</p>", "type": "rendered"}, "created_on": "2017-07-17T16:29:32.854889+00:00", "user": {"display_name": "Chris Billington", "uuid": "{e363c5a9-5075-4656-afb5-88bd6a6dceeb}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D"}, "html": {"href": "https://bitbucket.org/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/9238baf7300c41c0e7294db922899e6ad=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsCB-1.png"}}, "nickname": "cbillington", "type": "user", "account_id": "557058:cbf1bc43-1dc2-477b-9e25-1a8f40fd7ee3"}, "updated_on": null, "type": "issue_comment", "id": 38283641}, {"links": {"self": {"href": "data/repositories/labscript_suite/lyse/issues/29/comments/38286311.json"}, "html": {"href": "#!/labscript_suite/lyse/issues/29#comment-38286311"}}, "issue": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse/issues/29.json"}}, "type": "issue", "id": 29, "repository": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse.json"}, "html": {"href": "#!/labscript_suite/lyse"}, "avatar": {"href": "data/bytebucket.org/ravatar/{55eebdfe-43d1-4ae8-9049-50c55b295397}ts=249921"}}, "type": "repository", "name": "lyse", "full_name": "labscript_suite/lyse", "uuid": "{55eebdfe-43d1-4ae8-9049-50c55b295397}"}, "title": "speed up shot loading"}, "content": {"raw": "If you're posting code changes for discussion, it would be nice if you could post diffs for ease of seeing what has changed. You can run `hg diff somefile.py` from a terminal to get the diff text and post it here in a code environment with 'diff' as the language.\n\nI don't think allowing `update_row()` to take a `df_row_index` argument is a good idea *in general*. The reason is that since `update_row()` can be called from outside the main thread, any thread computing a row index and then calling `update_row()` runs the risk of the row index having changed before `update_row()` actually runs. By accepting the filepath instead, `update_row` ensures that it never has out of date information, as the filepath cannot change. \n\nHowever, if `add_files()` is the only caller using the `df_row_index` argument then that's fine, as `add_files()` itself runs in the main thread and so can be sure that nothing has changed in between its determination of the row number and the call to `update_row`. And performance optimisations are often compromises on making interfaces as pure as you'd like them to be, so I think I'm happy to accept this as a performance optimisation if it really is a significant speedup.\n\nOne way to prevent the race condition from biting anyone in the future if they use `df_row_index` argument from a non-main thread is to confirm that the filepath matches. You could have `add_files()` still pass in the filepath argument (rather than None),  and then have `update_row()` have a check like:\n\n\n```\n#!python\n\n# If df_row_index was used, check the row actually matches the filepath.\n# This might be the case if the caller was not in the main thread, which\n# is a situation vulnerable to race conditions:\nassert filepath == dataframe_row['filepath', '']\n```\n\nThis should be a factor of *n* faster than actually looking up the filepath, whilst ensuring race conditions don't cause invisible bugs in the future.\n\n\nI can see though how the line\n\n```\n#!python\n\ndf_row_index = np.where(self.dataframe['filepath'].values == filepath)\n```\n\ncould be a problem for performance. If you're calling it for every update, then it searches through the dataframe once for every file and so updating n files runs in quadratic time in n. \n\nWhen you say a factor of two, do you mean that the `update_row` function is two times faster without this search than with it? If so that's quite a speedup!\n\nThere are other ways to optimise lookups rather than searching. For example in sql databases you can \"index\" a column to speed up searches for rows that have a particular value. I don't know if pandas supports anything like this (googling for \"pandas index column\" obviously gives many irrelevant results), but we could also do that manually by maintaining a dictionary containing a filepath:row_index mapping that is updated whenever a row is added or removed (or rows re-ordered which we don't currently support but might in future).\n\nBut your solution with theabove  added check should be entirely sufficient. If there are similar slow searches through dataframes elsewhere in the code though that you hit upon, this other approach might be worth considering. Pandas people seem to care a lot about performance so it's possible that this functionality is built-in somewhere if you can work out what to type into google to find it.", "markup": "markdown", "html": "<p>If you're posting code changes for discussion, it would be nice if you could post diffs for ease of seeing what has changed. You can run <code>hg diff somefile.py</code> from a terminal to get the diff text and post it here in a code environment with 'diff' as the language.</p>\n<p>I don't think allowing <code>update_row()</code> to take a <code>df_row_index</code> argument is a good idea <em>in general</em>. The reason is that since <code>update_row()</code> can be called from outside the main thread, any thread computing a row index and then calling <code>update_row()</code> runs the risk of the row index having changed before <code>update_row()</code> actually runs. By accepting the filepath instead, <code>update_row</code> ensures that it never has out of date information, as the filepath cannot change. </p>\n<p>However, if <code>add_files()</code> is the only caller using the <code>df_row_index</code> argument then that's fine, as <code>add_files()</code> itself runs in the main thread and so can be sure that nothing has changed in between its determination of the row number and the call to <code>update_row</code>. And performance optimisations are often compromises on making interfaces as pure as you'd like them to be, so I think I'm happy to accept this as a performance optimisation if it really is a significant speedup.</p>\n<p>One way to prevent the race condition from biting anyone in the future if they use <code>df_row_index</code> argument from a non-main thread is to confirm that the filepath matches. You could have <code>add_files()</code> still pass in the filepath argument (rather than None),  and then have <code>update_row()</code> have a check like:</p>\n<div class=\"codehilite language-python\"><pre><span></span><span class=\"c1\"># If df_row_index was used, check the row actually matches the filepath.</span>\n<span class=\"c1\"># This might be the case if the caller was not in the main thread, which</span>\n<span class=\"c1\"># is a situation vulnerable to race conditions:</span>\n<span class=\"k\">assert</span> <span class=\"n\">filepath</span> <span class=\"o\">==</span> <span class=\"n\">dataframe_row</span><span class=\"p\">[</span><span class=\"s1\">&#39;filepath&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;&#39;</span><span class=\"p\">]</span>\n</pre></div>\n\n\n<p>This should be a factor of <em>n</em> faster than actually looking up the filepath, whilst ensuring race conditions don't cause invisible bugs in the future.</p>\n<p>I can see though how the line</p>\n<div class=\"codehilite language-python\"><pre><span></span><span class=\"n\">df_row_index</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"p\">[</span><span class=\"s1\">&#39;filepath&#39;</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">values</span> <span class=\"o\">==</span> <span class=\"n\">filepath</span><span class=\"p\">)</span>\n</pre></div>\n\n\n<p>could be a problem for performance. If you're calling it for every update, then it searches through the dataframe once for every file and so updating n files runs in quadratic time in n. </p>\n<p>When you say a factor of two, do you mean that the <code>update_row</code> function is two times faster without this search than with it? If so that's quite a speedup!</p>\n<p>There are other ways to optimise lookups rather than searching. For example in sql databases you can \"index\" a column to speed up searches for rows that have a particular value. I don't know if pandas supports anything like this (googling for \"pandas index column\" obviously gives many irrelevant results), but we could also do that manually by maintaining a dictionary containing a filepath:row_index mapping that is updated whenever a row is added or removed (or rows re-ordered which we don't currently support but might in future).</p>\n<p>But your solution with theabove  added check should be entirely sufficient. If there are similar slow searches through dataframes elsewhere in the code though that you hit upon, this other approach might be worth considering. Pandas people seem to care a lot about performance so it's possible that this functionality is built-in somewhere if you can work out what to type into google to find it.</p>", "type": "rendered"}, "created_on": "2017-07-17T18:40:03.474690+00:00", "user": {"display_name": "Chris Billington", "uuid": "{e363c5a9-5075-4656-afb5-88bd6a6dceeb}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D"}, "html": {"href": "https://bitbucket.org/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/9238baf7300c41c0e7294db922899e6ad=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsCB-1.png"}}, "nickname": "cbillington", "type": "user", "account_id": "557058:cbf1bc43-1dc2-477b-9e25-1a8f40fd7ee3"}, "updated_on": null, "type": "issue_comment", "id": 38286311}, {"links": {"self": {"href": "data/repositories/labscript_suite/lyse/issues/29/comments/38286751.json"}, "html": {"href": "#!/labscript_suite/lyse/issues/29#comment-38286751"}}, "issue": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse/issues/29.json"}}, "type": "issue", "id": 29, "repository": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse.json"}, "html": {"href": "#!/labscript_suite/lyse"}, "avatar": {"href": "data/bytebucket.org/ravatar/{55eebdfe-43d1-4ae8-9049-50c55b295397}ts=249921"}}, "type": "repository", "name": "lyse", "full_name": "labscript_suite/lyse", "uuid": "{55eebdfe-43d1-4ae8-9049-50c55b295397}"}, "title": "speed up shot loading"}, "content": {"raw": "Regarding the optimisation of `add_files`, using a set instead of searching individually is a great idea to speed things up, since set membership can be checked in constant time. \n\nI was confused at first though, since you turn filepaths into a set, but then later get it as a list from the dataframe again (presumable to get the correct order since sets don't preserve order). Could you give the set a different name (even as silly as appending `_set` to its name) to make it clear they have different purposes? I think the rest of that function looks fine, it's not clear to me why the other changes would result in any speed up but they are harmless. The usage of a set I can definitely see prevents us from having to search the whole dataframe for each shot.\n\nAnyway, it's all looking good. These are welcome changes and you should make a pull request about them, though I'm guessing you're waiting on your other pull request to be merged first.", "markup": "markdown", "html": "<p>Regarding the optimisation of <code>add_files</code>, using a set instead of searching individually is a great idea to speed things up, since set membership can be checked in constant time. </p>\n<p>I was confused at first though, since you turn filepaths into a set, but then later get it as a list from the dataframe again (presumable to get the correct order since sets don't preserve order). Could you give the set a different name (even as silly as appending <code>_set</code> to its name) to make it clear they have different purposes? I think the rest of that function looks fine, it's not clear to me why the other changes would result in any speed up but they are harmless. The usage of a set I can definitely see prevents us from having to search the whole dataframe for each shot.</p>\n<p>Anyway, it's all looking good. These are welcome changes and you should make a pull request about them, though I'm guessing you're waiting on your other pull request to be merged first.</p>", "type": "rendered"}, "created_on": "2017-07-17T18:57:37.551515+00:00", "user": {"display_name": "Chris Billington", "uuid": "{e363c5a9-5075-4656-afb5-88bd6a6dceeb}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D"}, "html": {"href": "https://bitbucket.org/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/9238baf7300c41c0e7294db922899e6ad=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsCB-1.png"}}, "nickname": "cbillington", "type": "user", "account_id": "557058:cbf1bc43-1dc2-477b-9e25-1a8f40fd7ee3"}, "updated_on": null, "type": "issue_comment", "id": 38286751}, {"links": {"self": {"href": "data/repositories/labscript_suite/lyse/issues/29/comments/38286891.json"}, "html": {"href": "#!/labscript_suite/lyse/issues/29#comment-38286891"}}, "issue": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse/issues/29.json"}}, "type": "issue", "id": 29, "repository": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse.json"}, "html": {"href": "#!/labscript_suite/lyse"}, "avatar": {"href": "data/bytebucket.org/ravatar/{55eebdfe-43d1-4ae8-9049-50c55b295397}ts=249921"}}, "type": "repository", "name": "lyse", "full_name": "labscript_suite/lyse", "uuid": "{55eebdfe-43d1-4ae8-9049-50c55b295397}"}, "title": "speed up shot loading"}, "content": {"raw": "Another thing you might look at is in `FileBox.incoming_buffer_loop()`, where we batch process HDF5 files in order to minimise the number of large dataframe concatenations. At the moment is is hard-coded to process 5 shots at a time, but you might want to experiment with making that larger, or a function of the number of shots already loaded, or something like that. 5 for me seemed to be the sweet spot when I was experimenting, but if dataframe concatenation is slow for very large numbers of shots then increasing this might be of benefit.\n\nOf course, profiling is the only way to know what bits of code should be looked at!", "markup": "markdown", "html": "<p>Another thing you might look at is in <code>FileBox.incoming_buffer_loop()</code>, where we batch process HDF5 files in order to minimise the number of large dataframe concatenations. At the moment is is hard-coded to process 5 shots at a time, but you might want to experiment with making that larger, or a function of the number of shots already loaded, or something like that. 5 for me seemed to be the sweet spot when I was experimenting, but if dataframe concatenation is slow for very large numbers of shots then increasing this might be of benefit.</p>\n<p>Of course, profiling is the only way to know what bits of code should be looked at!</p>", "type": "rendered"}, "created_on": "2017-07-17T19:04:18.496416+00:00", "user": {"display_name": "Chris Billington", "uuid": "{e363c5a9-5075-4656-afb5-88bd6a6dceeb}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D"}, "html": {"href": "https://bitbucket.org/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/9238baf7300c41c0e7294db922899e6ad=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsCB-1.png"}}, "nickname": "cbillington", "type": "user", "account_id": "557058:cbf1bc43-1dc2-477b-9e25-1a8f40fd7ee3"}, "updated_on": null, "type": "issue_comment", "id": 38286891}, {"links": {"self": {"href": "data/repositories/labscript_suite/lyse/issues/29/comments/38286941.json"}, "html": {"href": "#!/labscript_suite/lyse/issues/29#comment-38286941"}}, "issue": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse/issues/29.json"}}, "type": "issue", "id": 29, "repository": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse.json"}, "html": {"href": "#!/labscript_suite/lyse"}, "avatar": {"href": "data/bytebucket.org/ravatar/{55eebdfe-43d1-4ae8-9049-50c55b295397}ts=249921"}}, "type": "repository", "name": "lyse", "full_name": "labscript_suite/lyse", "uuid": "{55eebdfe-43d1-4ae8-9049-50c55b295397}"}, "title": "speed up shot loading"}, "content": {"raw": "Will definitely do diffs in the future (or create a branch on my fork).\n\nWell update_row is current called in 2 places add_files and in the analysis loop. So I don't think this is too bad.\nA check is probably a good idea will add that.\n\nYes as I wrote allready I'm importing/exporting 2000 shot dateframes as a whole(one file) and see a speed increase of a factor 2 there. And update_row is the only thing I'm editing currently(besides add_files) so I guess update_row becomes 2 times faster as a whole. I'm not quite sure how this affects loading 2000 files but would assume a speedup as well but smaller.\n\nPandas supports indexing(https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_index.html) and as I've read this also speeds up searches. You then reference rows in loc by that index like df.loc[filepath, column]. But i'm not that great with pandas so thats for someone else to do.\n\nThe main slowness I think here is the string comparison though.\n\nYes the set is unordered and well order seems to be better than chaos so thats why I'm later using a list.\n\nWith the other changes made I'm trying to reduce the amount of loops as they were not really needed.\n\nYes I'm waiting on the Update Dataframe in particular as they both touch the update_row function and I don't really want to end up with conflicts again. Also I'm sure there is more speed that can be gained so I'm not in a rush.\n\nThe incoming buffer I looked at butnothing really sticked out. I already wrote a workaround with the dataframe files that can be exported and imported. Is this something that could be of interest to others? Or is the goal to rather improve loading individual files?", "markup": "markdown", "html": "<p>Will definitely do diffs in the future (or create a branch on my fork).</p>\n<p>Well update_row is current called in 2 places add_files and in the analysis loop. So I don't think this is too bad.\nA check is probably a good idea will add that.</p>\n<p>Yes as I wrote allready I'm importing/exporting 2000 shot dateframes as a whole(one file) and see a speed increase of a factor 2 there. And update_row is the only thing I'm editing currently(besides add_files) so I guess update_row becomes 2 times faster as a whole. I'm not quite sure how this affects loading 2000 files but would assume a speedup as well but smaller.</p>\n<p>Pandas supports indexing(<a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_index.html\" rel=\"nofollow\" class=\"ap-connect-link\">https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_index.html</a>) and as I've read this also speeds up searches. You then reference rows in loc by that index like df.loc[filepath, column]. But i'm not that great with pandas so thats for someone else to do.</p>\n<p>The main slowness I think here is the string comparison though.</p>\n<p>Yes the set is unordered and well order seems to be better than chaos so thats why I'm later using a list.</p>\n<p>With the other changes made I'm trying to reduce the amount of loops as they were not really needed.</p>\n<p>Yes I'm waiting on the Update Dataframe in particular as they both touch the update_row function and I don't really want to end up with conflicts again. Also I'm sure there is more speed that can be gained so I'm not in a rush.</p>\n<p>The incoming buffer I looked at butnothing really sticked out. I already wrote a workaround with the dataframe files that can be exported and imported. Is this something that could be of interest to others? Or is the goal to rather improve loading individual files?</p>", "type": "rendered"}, "created_on": "2017-07-17T19:07:40.887662+00:00", "user": {"display_name": "Jan Werkmann", "uuid": "{44c4905c-2b90-4045-a5f1-652b8e228626}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B44c4905c-2b90-4045-a5f1-652b8e228626%7D"}, "html": {"href": "https://bitbucket.org/%7B44c4905c-2b90-4045-a5f1-652b8e228626%7D/"}, "avatar": {"href": "https://avatar-management--avatars.us-west-2.prod.public.atl-paas.net/557058:a70cc9cf-684e-4849-a61a-9ade4d7218b5/07e5095a-4741-4dc0-a462-9c7d455f961d/128"}}, "nickname": "PhyNerd", "type": "user", "account_id": "557058:a70cc9cf-684e-4849-a61a-9ade4d7218b5"}, "updated_on": null, "type": "issue_comment", "id": 38286941}, {"links": {"self": {"href": "data/repositories/labscript_suite/lyse/issues/29/comments/38287146.json"}, "html": {"href": "#!/labscript_suite/lyse/issues/29#comment-38287146"}}, "issue": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse/issues/29.json"}}, "type": "issue", "id": 29, "repository": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse.json"}, "html": {"href": "#!/labscript_suite/lyse"}, "avatar": {"href": "data/bytebucket.org/ravatar/{55eebdfe-43d1-4ae8-9049-50c55b295397}ts=249921"}}, "type": "repository", "name": "lyse", "full_name": "labscript_suite/lyse", "uuid": "{55eebdfe-43d1-4ae8-9049-50c55b295397}"}, "title": "speed up shot loading"}, "content": {"raw": "Hm, I wonder if that dataframe indexing helps. Yes, the string comparison is slow but I think only because it is being done once for every row in the dataframe. Whereas with the indexing if it's implemented well it should be something like a dictionary lookup, which is a hash-table and only does one string comparison (well, if you're lucky it does only one - it might do two or three but definitely not *n*). Definitely something I'll keep in mind for future performance improvements!", "markup": "markdown", "html": "<p>Hm, I wonder if that dataframe indexing helps. Yes, the string comparison is slow but I think only because it is being done once for every row in the dataframe. Whereas with the indexing if it's implemented well it should be something like a dictionary lookup, which is a hash-table and only does one string comparison (well, if you're lucky it does only one - it might do two or three but definitely not <em>n</em>). Definitely something I'll keep in mind for future performance improvements!</p>", "type": "rendered"}, "created_on": "2017-07-17T19:17:51.228555+00:00", "user": {"display_name": "Chris Billington", "uuid": "{e363c5a9-5075-4656-afb5-88bd6a6dceeb}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D"}, "html": {"href": "https://bitbucket.org/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/9238baf7300c41c0e7294db922899e6ad=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsCB-1.png"}}, "nickname": "cbillington", "type": "user", "account_id": "557058:cbf1bc43-1dc2-477b-9e25-1a8f40fd7ee3"}, "updated_on": null, "type": "issue_comment", "id": 38287146}, {"links": {"self": {"href": "data/repositories/labscript_suite/lyse/issues/29/comments/38287297.json"}, "html": {"href": "#!/labscript_suite/lyse/issues/29#comment-38287297"}}, "issue": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse/issues/29.json"}}, "type": "issue", "id": 29, "repository": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse.json"}, "html": {"href": "#!/labscript_suite/lyse"}, "avatar": {"href": "data/bytebucket.org/ravatar/{55eebdfe-43d1-4ae8-9049-50c55b295397}ts=249921"}}, "type": "repository", "name": "lyse", "full_name": "labscript_suite/lyse", "uuid": "{55eebdfe-43d1-4ae8-9049-50c55b295397}"}, "title": "speed up shot loading"}, "content": {"raw": "As for the exporting/importing thing, I'm not sure. What does your workaround do? It's for loading files in and out of lyse? I suppose \"load the exact state you had before\" can be cached without much difficulty, but how would you handle if a file has changed on disk or if different files are to be loaded? I'm open to all sorts of caches, but it's hard to get things like this right.\n\nOh by the way one more thing. At the moment we have some code running that makes sure we are not making calls to Qt outside the main thread. This is very important during development, as missing even a single call that is outside the main thread leads to extremely hard to debug bugs. However, this code runs every single time a Qt function is called, and so it runs a lot during `update_row()`.\n\nYou can disable it by putting somewhere at the top level of `__main__.py`:\n\n\n```\n#!python\n\nfrom qtutils import qtlock\nqtlock.enforce(False)\n```\nWould be interesting to see if it makes a difference.", "markup": "markdown", "html": "<p>As for the exporting/importing thing, I'm not sure. What does your workaround do? It's for loading files in and out of lyse? I suppose \"load the exact state you had before\" can be cached without much difficulty, but how would you handle if a file has changed on disk or if different files are to be loaded? I'm open to all sorts of caches, but it's hard to get things like this right.</p>\n<p>Oh by the way one more thing. At the moment we have some code running that makes sure we are not making calls to Qt outside the main thread. This is very important during development, as missing even a single call that is outside the main thread leads to extremely hard to debug bugs. However, this code runs every single time a Qt function is called, and so it runs a lot during <code>update_row()</code>.</p>\n<p>You can disable it by putting somewhere at the top level of <code>__main__.py</code>:</p>\n<div class=\"codehilite language-python\"><pre><span></span><span class=\"kn\">from</span> <span class=\"nn\">qtutils</span> <span class=\"kn\">import</span> <span class=\"n\">qtlock</span>\n<span class=\"n\">qtlock</span><span class=\"o\">.</span><span class=\"n\">enforce</span><span class=\"p\">(</span><span class=\"bp\">False</span><span class=\"p\">)</span>\n</pre></div>\n\n\n<p>Would be interesting to see if it makes a difference.</p>", "type": "rendered"}, "created_on": "2017-07-17T19:26:02.067630+00:00", "user": {"display_name": "Chris Billington", "uuid": "{e363c5a9-5075-4656-afb5-88bd6a6dceeb}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D"}, "html": {"href": "https://bitbucket.org/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/9238baf7300c41c0e7294db922899e6ad=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsCB-1.png"}}, "nickname": "cbillington", "type": "user", "account_id": "557058:cbf1bc43-1dc2-477b-9e25-1a8f40fd7ee3"}, "updated_on": "2017-07-17T19:26:39.695148+00:00", "type": "issue_comment", "id": 38287297}, {"links": {"self": {"href": "data/repositories/labscript_suite/lyse/issues/29/comments/38287335.json"}, "html": {"href": "#!/labscript_suite/lyse/issues/29#comment-38287335"}}, "issue": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse/issues/29.json"}}, "type": "issue", "id": 29, "repository": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse.json"}, "html": {"href": "#!/labscript_suite/lyse"}, "avatar": {"href": "data/bytebucket.org/ravatar/{55eebdfe-43d1-4ae8-9049-50c55b295397}ts=249921"}}, "type": "repository", "name": "lyse", "full_name": "labscript_suite/lyse", "uuid": "{55eebdfe-43d1-4ae8-9049-50c55b295397}"}, "title": "speed up shot loading"}, "content": {"raw": "Oh, actually, reading the code for the `qtlock` thing, it looks like the checks only happen in non-main threads, so it won't speed up anything in `update_row()` since `update_row()` is in the main thread. It might speed up other code though, but other code is not the bottleneck!", "markup": "markdown", "html": "<p>Oh, actually, reading the code for the <code>qtlock</code> thing, it looks like the checks only happen in non-main threads, so it won't speed up anything in <code>update_row()</code> since <code>update_row()</code> is in the main thread. It might speed up other code though, but other code is not the bottleneck!</p>", "type": "rendered"}, "created_on": "2017-07-17T19:28:52.250271+00:00", "user": {"display_name": "Chris Billington", "uuid": "{e363c5a9-5075-4656-afb5-88bd6a6dceeb}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D"}, "html": {"href": "https://bitbucket.org/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/9238baf7300c41c0e7294db922899e6ad=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsCB-1.png"}}, "nickname": "cbillington", "type": "user", "account_id": "557058:cbf1bc43-1dc2-477b-9e25-1a8f40fd7ee3"}, "updated_on": null, "type": "issue_comment", "id": 38287335}, {"links": {"self": {"href": "data/repositories/labscript_suite/lyse/issues/29/comments/38288240.json"}, "html": {"href": "#!/labscript_suite/lyse/issues/29#comment-38288240"}}, "issue": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse/issues/29.json"}}, "type": "issue", "id": 29, "repository": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse.json"}, "html": {"href": "#!/labscript_suite/lyse"}, "avatar": {"href": "data/bytebucket.org/ravatar/{55eebdfe-43d1-4ae8-9049-50c55b295397}ts=249921"}}, "type": "repository", "name": "lyse", "full_name": "labscript_suite/lyse", "uuid": "{55eebdfe-43d1-4ae8-9049-50c55b295397}"}, "title": "speed up shot loading"}, "content": {"raw": "Currently I'm not checking if the files still exist/have changed etc. I'm just splitting tha dataframe by sequences and exporting it into a hdf5 file on press of a button. The hdf5 file however to net get confused is saved with the file ending .df . I'm using DataFrame.to_hdf and pandas.read_hdf for this.\n\nThis is mainly useful when measuring lots of sequences over night. We then export the dataframe(s) and load in the individual sequences one after the other as this is a lot faster than loading the files. Also after things are done we overwrite the old dataframe with a updated version. \nAs loading a dataframe like this only takes up about 5 seconds one can take a quick look at old data or quickly run some new routines without the hassle of waiting for up to 15 minutes.\n\n\nAlso here is the result of your profiler running on add_files:\n![example.png](data/bitbucket.org/repo/BMBAeq/images/4234763019-example.png)\nHaven't really looked at it much but if I interpret it correctly scientific_notation and get_model_row_by_filepath seems to be slow.", "markup": "markdown", "html": "<p>Currently I'm not checking if the files still exist/have changed etc. I'm just splitting tha dataframe by sequences and exporting it into a hdf5 file on press of a button. The hdf5 file however to net get confused is saved with the file ending .df . I'm using DataFrame.to_hdf and pandas.read_hdf for this.</p>\n<p>This is mainly useful when measuring lots of sequences over night. We then export the dataframe(s) and load in the individual sequences one after the other as this is a lot faster than loading the files. Also after things are done we overwrite the old dataframe with a updated version. \nAs loading a dataframe like this only takes up about 5 seconds one can take a quick look at old data or quickly run some new routines without the hassle of waiting for up to 15 minutes.</p>\n<p>Also here is the result of your profiler running on add_files:\n<img alt=\"example.png\" src=\"data/bitbucket.org/repo/BMBAeq/images/4234763019-example.png\" />\nHaven't really looked at it much but if I interpret it correctly scientific_notation and get_model_row_by_filepath seems to be slow.</p>", "type": "rendered"}, "created_on": "2017-07-17T20:24:22.912917+00:00", "user": {"display_name": "Jan Werkmann", "uuid": "{44c4905c-2b90-4045-a5f1-652b8e228626}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B44c4905c-2b90-4045-a5f1-652b8e228626%7D"}, "html": {"href": "https://bitbucket.org/%7B44c4905c-2b90-4045-a5f1-652b8e228626%7D/"}, "avatar": {"href": "https://avatar-management--avatars.us-west-2.prod.public.atl-paas.net/557058:a70cc9cf-684e-4849-a61a-9ade4d7218b5/07e5095a-4741-4dc0-a462-9c7d455f961d/128"}}, "nickname": "PhyNerd", "type": "user", "account_id": "557058:a70cc9cf-684e-4849-a61a-9ade4d7218b5"}, "updated_on": null, "type": "issue_comment", "id": 38288240}, {"links": {"self": {"href": "data/repositories/labscript_suite/lyse/issues/29/comments/38309117.json"}, "html": {"href": "#!/labscript_suite/lyse/issues/29#comment-38309117"}}, "issue": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse/issues/29.json"}}, "type": "issue", "id": 29, "repository": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse.json"}, "html": {"href": "#!/labscript_suite/lyse"}, "avatar": {"href": "data/bytebucket.org/ravatar/{55eebdfe-43d1-4ae8-9049-50c55b295397}ts=249921"}}, "type": "repository", "name": "lyse", "full_name": "labscript_suite/lyse", "uuid": "{55eebdfe-43d1-4ae8-9049-50c55b295397}"}, "title": "speed up shot loading"}, "content": {"raw": "If someones interested my changes are now over at [File-loading-Performance](#!/PhyNerd/lyse/branch/File-loading-Performance). I've improved update_row, add_files and getmodel_row_by_filepath so far.\n\nI think there is still a lot to gain with scientific_notation and update_row so I'll be waiting until that is done before creating a pullrequest.", "markup": "markdown", "html": "<p>If someones interested my changes are now over at <a data-is-external-link=\"true\" href=\"#!/PhyNerd/lyse/branch/File-loading-Performance\" rel=\"nofollow\">File-loading-Performance</a>. I've improved update_row, add_files and getmodel_row_by_filepath so far.</p>\n<p>I think there is still a lot to gain with scientific_notation and update_row so I'll be waiting until that is done before creating a pullrequest.</p>", "type": "rendered"}, "created_on": "2017-07-18T16:18:48.029913+00:00", "user": {"display_name": "Jan Werkmann", "uuid": "{44c4905c-2b90-4045-a5f1-652b8e228626}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B44c4905c-2b90-4045-a5f1-652b8e228626%7D"}, "html": {"href": "https://bitbucket.org/%7B44c4905c-2b90-4045-a5f1-652b8e228626%7D/"}, "avatar": {"href": "https://avatar-management--avatars.us-west-2.prod.public.atl-paas.net/557058:a70cc9cf-684e-4849-a61a-9ade4d7218b5/07e5095a-4741-4dc0-a462-9c7d455f961d/128"}}, "nickname": "PhyNerd", "type": "user", "account_id": "557058:a70cc9cf-684e-4849-a61a-9ade4d7218b5"}, "updated_on": null, "type": "issue_comment", "id": 38309117}, {"links": {"self": {"href": "data/repositories/labscript_suite/lyse/issues/29/comments/38311976.json"}, "html": {"href": "#!/labscript_suite/lyse/issues/29#comment-38311976"}}, "issue": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse/issues/29.json"}}, "type": "issue", "id": 29, "repository": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse.json"}, "html": {"href": "#!/labscript_suite/lyse"}, "avatar": {"href": "data/bytebucket.org/ravatar/{55eebdfe-43d1-4ae8-9049-50c55b295397}ts=249921"}}, "type": "repository", "name": "lyse", "full_name": "labscript_suite/lyse", "uuid": "{55eebdfe-43d1-4ae8-9049-50c55b295397}"}, "title": "speed up shot loading"}, "content": {"raw": "There is also about 30% of `update_row()` missing in the percentages, which makes me think that that 30% is made up of many small things none of which is above 5% of run time (BProfile has a default threshold of 5% but you can set it smaller to see more detail). That as well as the visible places it is spending time make me think that unfortunately we're running out of places to optimise without a more radical change. So chipping away at scientific_notation() could be good (possible low hanging fruit - move the constants to the global namespace rather than defining them within the function?), but there's diminishing returns in optimising things more as they are.\n\nSo a more radical change would be to look into how Qt models work and subclass it to be backed by the dataframe itself, with the tooltips and scientific notation all being computed lazily only when the method to get that data was called (indicating the column was actually visible or the user had actually moused-over for a tooltip) and possibly  cached. This was originally suggested by @philipstarkey when we began porting to Qt, and I don't recall why I didn't go for it. In any case the code is organised well enough that the change would be isolated to the `DataFrameModel` class (the name maybe hints that I was initially going to go with phil's suggestion but gave up because it looked too tricky?).\n\nThat could be something to look into for the future, but these optimisations should be included now anyway!\n\nI'm a little skeptical about the profiling results, surprised to not see much in the way of Qt calls in there. Maybe Qt is just really fast, but it's also the case that sometimes the profiler doesn't catch certain things - like I'm not sure if the creation of constants like the dictionaries and string constants in `scientific_notation` would appear in the profiling results. I have another profiling tool that measures what percentage of the time is spent on each *line* of a given file rather than profiling in terms of function calls, which I have found to be super useful, but at the moment this tool is a pile of platform-specific hacks rather than something I could distribute. But I'll think about getting it into shape so we can do some better profiling since performance is so important to people with so many shots.\n\nI think you should feel free to make a pull request even if you think something isn't \"ready\" yet. Pull requests are more visible in the bitbucket UI and are a nice way to see what things are \"in progress\" from other people's forks. Pull requests have the diff and list of commits visible and have nested comment threads (which issue threads frustratingly lack). They also have the source branch listed so people can see where to pull from for testing without you having to tell us. Just mention in the pull request that you're still looking for feedback and we won't merge - even if you don't mention anything, most things won't be merged without some comments anyway about testing etc. We can always just reject a pull request if an approach is abandoned.", "markup": "markdown", "html": "<p>There is also about 30% of <code>update_row()</code> missing in the percentages, which makes me think that that 30% is made up of many small things none of which is above 5% of run time (BProfile has a default threshold of 5% but you can set it smaller to see more detail). That as well as the visible places it is spending time make me think that unfortunately we're running out of places to optimise without a more radical change. So chipping away at scientific_notation() could be good (possible low hanging fruit - move the constants to the global namespace rather than defining them within the function?), but there's diminishing returns in optimising things more as they are.</p>\n<p>So a more radical change would be to look into how Qt models work and subclass it to be backed by the dataframe itself, with the tooltips and scientific notation all being computed lazily only when the method to get that data was called (indicating the column was actually visible or the user had actually moused-over for a tooltip) and possibly  cached. This was originally suggested by @philipstarkey when we began porting to Qt, and I don't recall why I didn't go for it. In any case the code is organised well enough that the change would be isolated to the <code>DataFrameModel</code> class (the name maybe hints that I was initially going to go with phil's suggestion but gave up because it looked too tricky?).</p>\n<p>That could be something to look into for the future, but these optimisations should be included now anyway!</p>\n<p>I'm a little skeptical about the profiling results, surprised to not see much in the way of Qt calls in there. Maybe Qt is just really fast, but it's also the case that sometimes the profiler doesn't catch certain things - like I'm not sure if the creation of constants like the dictionaries and string constants in <code>scientific_notation</code> would appear in the profiling results. I have another profiling tool that measures what percentage of the time is spent on each <em>line</em> of a given file rather than profiling in terms of function calls, which I have found to be super useful, but at the moment this tool is a pile of platform-specific hacks rather than something I could distribute. But I'll think about getting it into shape so we can do some better profiling since performance is so important to people with so many shots.</p>\n<p>I think you should feel free to make a pull request even if you think something isn't \"ready\" yet. Pull requests are more visible in the bitbucket UI and are a nice way to see what things are \"in progress\" from other people's forks. Pull requests have the diff and list of commits visible and have nested comment threads (which issue threads frustratingly lack). They also have the source branch listed so people can see where to pull from for testing without you having to tell us. Just mention in the pull request that you're still looking for feedback and we won't merge - even if you don't mention anything, most things won't be merged without some comments anyway about testing etc. We can always just reject a pull request if an approach is abandoned.</p>", "type": "rendered"}, "created_on": "2017-07-18T18:41:32.609399+00:00", "user": {"display_name": "Chris Billington", "uuid": "{e363c5a9-5075-4656-afb5-88bd6a6dceeb}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D"}, "html": {"href": "https://bitbucket.org/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/9238baf7300c41c0e7294db922899e6ad=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsCB-1.png"}}, "nickname": "cbillington", "type": "user", "account_id": "557058:cbf1bc43-1dc2-477b-9e25-1a8f40fd7ee3"}, "updated_on": null, "type": "issue_comment", "id": 38311976}, {"links": {"self": {"href": "data/repositories/labscript_suite/lyse/issues/29/comments/38312522.json"}, "html": {"href": "#!/labscript_suite/lyse/issues/29#comment-38312522"}}, "issue": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse/issues/29.json"}}, "type": "issue", "id": 29, "repository": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse.json"}, "html": {"href": "#!/labscript_suite/lyse"}, "avatar": {"href": "data/bytebucket.org/ravatar/{55eebdfe-43d1-4ae8-9049-50c55b295397}ts=249921"}}, "type": "repository", "name": "lyse", "full_name": "labscript_suite/lyse", "uuid": "{55eebdfe-43d1-4ae8-9049-50c55b295397}"}, "title": "speed up shot loading"}, "content": {"raw": "Moving the variables to the global namespace didn't do much. As well as removing the try and ecept and replacing it with if else.\n\n```\n#!python\n\nif exponent % 3 == 0 and -25 < exponent < 25:\n...\nelse:\n```\n I guess scientific notation is just that dominant as it's called lots of times.\n\n\nA \"significant\" gain however can be achieved by removing resizeRowToContents in add_files but the gain isn't great enough to justify removing it (1 second).\n\nThe main time consuming factor in update_row seems to be the creation of all the QtGui.QStandardItems. This leads me to believe that rewriting Qt's modle would most likely hold a much larger increase in speed. However pandas and Qt are both not my strong suite  as I've had my first contact with them last month. \n\nLuckily performance problems with many shots are confined to lyse for the moment. But your profiling tool is/was definitely a big help finding lines that need improvement.\n\nI'll wait for the update dataframe pull request to go through and then I'll create the pull request. The repos were quite inactive in the last weeks so I'm not really expecting much response at the moment anyway.", "markup": "markdown", "html": "<p>Moving the variables to the global namespace didn't do much. As well as removing the try and ecept and replacing it with if else.</p>\n<div class=\"codehilite language-python\"><pre><span></span><span class=\"k\">if</span> <span class=\"n\">exponent</span> <span class=\"o\">%</span> <span class=\"mi\">3</span> <span class=\"o\">==</span> <span class=\"mi\">0</span> <span class=\"ow\">and</span> <span class=\"o\">-</span><span class=\"mi\">25</span> <span class=\"o\">&lt;</span> <span class=\"n\">exponent</span> <span class=\"o\">&lt;</span> <span class=\"mi\">25</span><span class=\"p\">:</span>\n<span class=\"o\">...</span>\n<span class=\"k\">else</span><span class=\"p\">:</span>\n</pre></div>\n\n\n<p>I guess scientific notation is just that dominant as it's called lots of times.</p>\n<p>A \"significant\" gain however can be achieved by removing resizeRowToContents in add_files but the gain isn't great enough to justify removing it (1 second).</p>\n<p>The main time consuming factor in update_row seems to be the creation of all the QtGui.QStandardItems. This leads me to believe that rewriting Qt's modle would most likely hold a much larger increase in speed. However pandas and Qt are both not my strong suite  as I've had my first contact with them last month. </p>\n<p>Luckily performance problems with many shots are confined to lyse for the moment. But your profiling tool is/was definitely a big help finding lines that need improvement.</p>\n<p>I'll wait for the update dataframe pull request to go through and then I'll create the pull request. The repos were quite inactive in the last weeks so I'm not really expecting much response at the moment anyway.</p>", "type": "rendered"}, "created_on": "2017-07-18T19:09:42.997892+00:00", "user": {"display_name": "Jan Werkmann", "uuid": "{44c4905c-2b90-4045-a5f1-652b8e228626}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B44c4905c-2b90-4045-a5f1-652b8e228626%7D"}, "html": {"href": "https://bitbucket.org/%7B44c4905c-2b90-4045-a5f1-652b8e228626%7D/"}, "avatar": {"href": "https://avatar-management--avatars.us-west-2.prod.public.atl-paas.net/557058:a70cc9cf-684e-4849-a61a-9ade4d7218b5/07e5095a-4741-4dc0-a462-9c7d455f961d/128"}}, "nickname": "PhyNerd", "type": "user", "account_id": "557058:a70cc9cf-684e-4849-a61a-9ade4d7218b5"}, "updated_on": null, "type": "issue_comment", "id": 38312522}], "page": 1, "size": 15}