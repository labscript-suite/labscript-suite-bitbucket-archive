{"priority": "minor", "kind": "enhancement", "repository": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse.json"}, "html": {"href": "#!/labscript_suite/lyse"}, "avatar": {"href": "data/bytebucket.org/ravatar/{55eebdfe-43d1-4ae8-9049-50c55b295397}ts=249921"}}, "type": "repository", "name": "lyse", "full_name": "labscript_suite/lyse", "uuid": "{55eebdfe-43d1-4ae8-9049-50c55b295397}"}, "links": {"attachments": {"href": "data/repositories/labscript_suite/lyse/issues/44/attachments_page=1.json"}, "self": {"href": "data/repositories/labscript_suite/lyse/issues/44.json"}, "watch": {"href": "https://api.bitbucket.org/2.0/repositories/labscript_suite/lyse/issues/44/watch"}, "comments": {"href": "data/repositories/labscript_suite/lyse/issues/44/comments_page=1.json"}, "html": {"href": "#!/labscript_suite/lyse/issues/44/allow-save_result_array-s-methods-to-use"}, "vote": {"href": "https://api.bitbucket.org/2.0/repositories/labscript_suite/lyse/issues/44/vote"}}, "reporter": {"display_name": "David Meyer", "uuid": "{8df655d6-8661-4957-b4ba-669cef19bf2d}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B8df655d6-8661-4957-b4ba-669cef19bf2d%7D"}, "html": {"href": "https://bitbucket.org/%7B8df655d6-8661-4957-b4ba-669cef19bf2d%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/2a4efe81a17e5aeccdbc4ef2ca2cc463d=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsDM-0.png"}}, "nickname": "dihm", "type": "user", "account_id": "557058:411f7712-e1fa-438c-b578-0359da2d9f06"}, "title": "Allow save_result_array(s) methods to use h5 dataset compression filters", "component": null, "votes": 0, "watches": 1, "content": {"raw": "We've recently had an experiment get up to speed and start producing a prodigious amount of data. In researching how to deal with it, I stumbled on to hdf5 dataset compression. It gives a performance hit to read/writes for moderate size reductions while being essentially transparent in use, so long as the compression is specified when the dataset is first created. \r\n\r\nIt would be nice if large, summary datasets produced in lyse could be stored compressed from the outset. Because of performance considerations, we don't want it to be default, so it should be configurable on individual data saves as well.\r\n\r\n My initial thought was to simply pass through kwargs to the create_dataset function in both save_result_arrays and save_result_array. Thoughts?\r\n\r\n```\r\n#!diff\r\n\r\n@@ -209,7 +209,8 @@\r\n                 _updated_data[self.h5_path] = {}\r\n             _updated_data[self.h5_path][str(self.group), name] = value\r\n \r\n-    def save_result_array(self, name, data, group=None, overwrite=True, keep_attrs=False):\r\n+    def save_result_array(self, name, data, group=None, \r\n+                          overwrite=True, keep_attrs=False, **compress_args):\r\n         if self.no_write:\r\n             raise Exception('This run is read-only. '\r\n                             'You can\\'t save results to runs through a '\r\n@@ -233,7 +234,7 @@\r\n                 else:\r\n                     raise Exception('Dataset %s exists. Use overwrite=True to overwrite.' % \r\n                                      group + '/' + name)\r\n-            h5_file[group].create_dataset(name, data=data)\r\n+            h5_file[group].create_dataset(name, data=data, **compress_args)\r\n             for key, val in attrs.items():\r\n                 h5_file[group][name].attrs[key] = val\r\n \r\n@@ -264,7 +265,7 @@\r\n                 self.save_result(name, value[0], **kwargs)\r\n                 self.save_result('u_' + name, value[1], **kwargs)\r\n \r\n-    def save_result_arrays(self, *args):\r\n+    def save_result_arrays(self, *args, **compress_args):\r\n         names = args[::2]\r\n         values = args[1::2]\r\n         for name, value in zip(names, values):\r\n@@ -268,7 +269,7 @@\r\n         names = args[::2]\r\n         values = args[1::2]\r\n         for name, value in zip(names, values):\r\n-            self.save_result_array(name, value)\r\n+            self.save_result_array(name, value, **compress_args)\r\n     \r\n     def get_image(self,orientation,label,image):\r\n         with h5py.File(self.h5_path) as h5_file:\r\n\r\n```", "markup": "markdown", "html": "<p>We've recently had an experiment get up to speed and start producing a prodigious amount of data. In researching how to deal with it, I stumbled on to hdf5 dataset compression. It gives a performance hit to read/writes for moderate size reductions while being essentially transparent in use, so long as the compression is specified when the dataset is first created. </p>\n<p>It would be nice if large, summary datasets produced in lyse could be stored compressed from the outset. Because of performance considerations, we don't want it to be default, so it should be configurable on individual data saves as well.</p>\n<p>My initial thought was to simply pass through kwargs to the create_dataset function in both save_result_arrays and save_result_array. Thoughts?</p>\n<div class=\"codehilite language-diff\"><pre><span></span><span class=\"gu\">@@ -209,7 +209,8 @@</span>\n                 _updated_data[self.h5_path] = {}\n             _updated_data[self.h5_path][str(self.group), name] = value\n\n<span class=\"gd\">-    def save_result_array(self, name, data, group=None, overwrite=True, keep_attrs=False):</span>\n<span class=\"gi\">+    def save_result_array(self, name, data, group=None, </span>\n<span class=\"gi\">+                          overwrite=True, keep_attrs=False, **compress_args):</span>\n         if self.no_write:\n             raise Exception(&#39;This run is read-only. &#39;\n                             &#39;You can\\&#39;t save results to runs through a &#39;\n<span class=\"gu\">@@ -233,7 +234,7 @@</span>\n                 else:\n                     raise Exception(&#39;Dataset %s exists. Use overwrite=True to overwrite.&#39; % \n                                      group + &#39;/&#39; + name)\n<span class=\"gd\">-            h5_file[group].create_dataset(name, data=data)</span>\n<span class=\"gi\">+            h5_file[group].create_dataset(name, data=data, **compress_args)</span>\n             for key, val in attrs.items():\n                 h5_file[group][name].attrs[key] = val\n\n<span class=\"gu\">@@ -264,7 +265,7 @@</span>\n                 self.save_result(name, value[0], **kwargs)\n                 self.save_result(&#39;u_&#39; + name, value[1], **kwargs)\n\n<span class=\"gd\">-    def save_result_arrays(self, *args):</span>\n<span class=\"gi\">+    def save_result_arrays(self, *args, **compress_args):</span>\n         names = args[::2]\n         values = args[1::2]\n         for name, value in zip(names, values):\n<span class=\"gu\">@@ -268,7 +269,7 @@</span>\n         names = args[::2]\n         values = args[1::2]\n         for name, value in zip(names, values):\n<span class=\"gd\">-            self.save_result_array(name, value)</span>\n<span class=\"gi\">+            self.save_result_array(name, value, **compress_args)</span>\n\n     def get_image(self,orientation,label,image):\n         with h5py.File(self.h5_path) as h5_file:\n</pre></div>", "type": "rendered"}, "assignee": null, "state": "new", "version": null, "edited_on": null, "created_on": "2018-11-13T20:42:58.483369+00:00", "milestone": null, "updated_on": "2018-11-13T20:42:58.483369+00:00", "type": "issue", "id": 44}