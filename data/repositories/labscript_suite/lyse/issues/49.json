{"priority": "major", "kind": "bug", "repository": {"links": {"self": {"href": "data/repositories/labscript_suite/lyse.json"}, "html": {"href": "#!/labscript_suite/lyse"}, "avatar": {"href": "data/bytebucket.org/ravatar/{55eebdfe-43d1-4ae8-9049-50c55b295397}ts=249921"}}, "type": "repository", "name": "lyse", "full_name": "labscript_suite/lyse", "uuid": "{55eebdfe-43d1-4ae8-9049-50c55b295397}"}, "links": {"attachments": {"href": "data/repositories/labscript_suite/lyse/issues/49/attachments_page=1.json"}, "self": {"href": "data/repositories/labscript_suite/lyse/issues/49.json"}, "watch": {"href": "https://api.bitbucket.org/2.0/repositories/labscript_suite/lyse/issues/49/watch"}, "comments": {"href": "data/repositories/labscript_suite/lyse/issues/49/comments_page=1.json"}, "html": {"href": "#!/labscript_suite/lyse/issues/49/subprocess-deadlock-with-multiple-threads"}, "vote": {"href": "https://api.bitbucket.org/2.0/repositories/labscript_suite/lyse/issues/49/vote"}}, "reporter": {"display_name": "Chris Billington", "uuid": "{e363c5a9-5075-4656-afb5-88bd6a6dceeb}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D"}, "html": {"href": "https://bitbucket.org/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/9238baf7300c41c0e7294db922899e6ad=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsCB-1.png"}}, "nickname": "cbillington", "type": "user", "account_id": "557058:cbf1bc43-1dc2-477b-9e25-1a8f40fd7ee3"}, "title": "Subprocess deadlock with multiple threads", "component": null, "votes": 0, "watches": 1, "content": {"raw": "I'm ocassionally seeing the [analysislib-mloop](https://bitbucket.org/rpanderson/analysislib-mloop/) analysis subprocess hanging when communicating with runmanager. It's not spinning the CPU, it doesn't raise an exception, it's just hanging, so it looks like a deadlock. If I add printlines around the zeromq calls to communicate with runmanager, it's clear that the code is either hanging in the zmq poll call to hear back from runmanager, or it's hanging in the subsequent print statement. I suspect the print statement, because if I disable output redirection for lyse routines (and look at my printlines in the terminal instead), then I don't see the hang.\r\n\r\nIt doesn't have anything to do with the deadlock in [labscript_utils PR 84](#!/labscript_suite/labscript_utils/pull-requests/84/), because I observe it without that PR merged in.\r\n\r\nI have only observed it when using the Gaussian process MLOOP controller. Since I doubt it has to do with the actual optimisation algorithm, this is probably because the Gaussian process is more computationally intensive, so this affects timing.\r\n\r\nI am guessing the output redirection isn't as threadsafe as I thought it was. Threads are sharing a zeromq socket, though they serialise access to it with a lock. Nonetheless perhaps this is not enough.\r\n\r\nWill pull out a debugger to inspect the Python process whilst hanging to see where it's at.", "markup": "markdown", "html": "<p>I'm ocassionally seeing the <a data-is-external-link=\"true\" href=\"https://bitbucket.org/rpanderson/analysislib-mloop/\" rel=\"nofollow\">analysislib-mloop</a> analysis subprocess hanging when communicating with runmanager. It's not spinning the CPU, it doesn't raise an exception, it's just hanging, so it looks like a deadlock. If I add printlines around the zeromq calls to communicate with runmanager, it's clear that the code is either hanging in the zmq poll call to hear back from runmanager, or it's hanging in the subsequent print statement. I suspect the print statement, because if I disable output redirection for lyse routines (and look at my printlines in the terminal instead), then I don't see the hang.</p>\n<p>It doesn't have anything to do with the deadlock in <a data-is-external-link=\"true\" href=\"#!/labscript_suite/labscript_utils/pull-requests/84/\" rel=\"nofollow\">labscript_utils PR 84</a>, because I observe it without that PR merged in.</p>\n<p>I have only observed it when using the Gaussian process MLOOP controller. Since I doubt it has to do with the actual optimisation algorithm, this is probably because the Gaussian process is more computationally intensive, so this affects timing.</p>\n<p>I am guessing the output redirection isn't as threadsafe as I thought it was. Threads are sharing a zeromq socket, though they serialise access to it with a lock. Nonetheless perhaps this is not enough.</p>\n<p>Will pull out a debugger to inspect the Python process whilst hanging to see where it's at.</p>", "type": "rendered"}, "assignee": {"display_name": "Chris Billington", "uuid": "{e363c5a9-5075-4656-afb5-88bd6a6dceeb}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D"}, "html": {"href": "https://bitbucket.org/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/9238baf7300c41c0e7294db922899e6ad=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsCB-1.png"}}, "nickname": "cbillington", "type": "user", "account_id": "557058:cbf1bc43-1dc2-477b-9e25-1a8f40fd7ee3"}, "state": "resolved", "version": null, "edited_on": null, "created_on": "2019-06-05T22:51:39.707854+00:00", "milestone": null, "updated_on": "2019-06-09T16:37:11.926023+00:00", "type": "issue", "id": 49}