{"links": {"self": {"href": "data/repositories/labscript_suite/lyse/pullrequests/29/comments/43054975.json"}, "html": {"href": "#!/labscript_suite/lyse/pull-requests/29/_/diff#comment-43054975"}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 29, "links": {"self": {"href": "data/repositories/labscript_suite/lyse/pullrequests/29.json"}, "html": {"href": "#!/labscript_suite/lyse/pull-requests/29"}}, "title": "Cross routine caching"}, "content": {"raw": "Ok, so you were totally right on this one: your performance issues are because of the large number of file operations (when zlock is on localhost - when it's not I suspect it would dominate). The reason data transfer doesn't dominate is because your images are really tiny! 51 \u00d7 51 pixels!\n\nAny smaller and we would say \"just put it in the dataframe\", any larger and reducing the number of filesystem operations would not be much benefit since actual data transfer time would dominate.\n\nIn fact I tried putting the data in the dataframe - in your singleshot analysis I added a `save_result('im1', image1)` etc, and then in the multishot routine got the images with:\n\n```\n#!python\n\ndf = lyse.data()\nimage1 = dict(zip(df['filepath'], df['singleshot_routine_name', 'im1']))\n```\nwhich was super fast (and which led to discovering bugs about arrays as saved results as fixed in pull request #31).\n\nSo whether the cache is the dataframe or not, it looks like a cache is definitely the way to go. And since if you did this to many different images per shot, it would make the dataframe larger than neccesary and slow down multishot routines that were not interested in the images but would nonetheless need to pay the data transfer (39 MB in your example for 2 tiny images times 2000 shots) and pickling cost of them being in the dataframe, I think a separate cache is also the way to go.\n\nSo I'll have some nitpicks about the exact implementation of this pull request shortly - and also possible performance improvements for people whose images are not so small who want to use a cache, because for them pickling the images is a real waste - there are more efficient ways to serialise numpy arrays, and for you this is only working well because they're so tiny.\n\nBut I'm broadly in agreement with the approach here.", "markup": "markdown", "html": "<p>Ok, so you were totally right on this one: your performance issues are because of the large number of file operations (when zlock is on localhost - when it's not I suspect it would dominate). The reason data transfer doesn't dominate is because your images are really tiny! 51 \u00d7 51 pixels!</p>\n<p>Any smaller and we would say \"just put it in the dataframe\", any larger and reducing the number of filesystem operations would not be much benefit since actual data transfer time would dominate.</p>\n<p>In fact I tried putting the data in the dataframe - in your singleshot analysis I added a <code>save_result('im1', image1)</code> etc, and then in the multishot routine got the images with:</p>\n<div class=\"codehilite language-python\"><pre><span></span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">lyse</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">()</span>\n<span class=\"n\">image1</span> <span class=\"o\">=</span> <span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">[</span><span class=\"s1\">&#39;filepath&#39;</span><span class=\"p\">],</span> <span class=\"n\">df</span><span class=\"p\">[</span><span class=\"s1\">&#39;singleshot_routine_name&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;im1&#39;</span><span class=\"p\">]))</span>\n</pre></div>\n\n\n<p>which was super fast (and which led to discovering bugs about arrays as saved results as fixed in <a href=\"#!/labscript_suite/lyse/pull-requests/31/fix-a-bug-in-updating-the-dataframe\" rel=\"nofollow\" class=\"ap-connect-link\">pull request #31</a>).</p>\n<p>So whether the cache is the dataframe or not, it looks like a cache is definitely the way to go. And since if you did this to many different images per shot, it would make the dataframe larger than neccesary and slow down multishot routines that were not interested in the images but would nonetheless need to pay the data transfer (39 MB in your example for 2 tiny images times 2000 shots) and pickling cost of them being in the dataframe, I think a separate cache is also the way to go.</p>\n<p>So I'll have some nitpicks about the exact implementation of this pull request shortly - and also possible performance improvements for people whose images are not so small who want to use a cache, because for them pickling the images is a real waste - there are more efficient ways to serialise numpy arrays, and for you this is only working well because they're so tiny.</p>\n<p>But I'm broadly in agreement with the approach here.</p>", "type": "rendered"}, "created_on": "2017-08-17T21:07:42.335893+00:00", "user": {"display_name": "Chris Billington", "uuid": "{e363c5a9-5075-4656-afb5-88bd6a6dceeb}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D"}, "html": {"href": "https://bitbucket.org/%7Be363c5a9-5075-4656-afb5-88bd6a6dceeb%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/9238baf7300c41c0e7294db922899e6ad=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsCB-1.png"}}, "nickname": "cbillington", "type": "user", "account_id": "557058:cbf1bc43-1dc2-477b-9e25-1a8f40fd7ee3"}, "updated_on": "2017-08-17T21:09:18.063402+00:00", "type": "pullrequest_comment", "id": 43054975}